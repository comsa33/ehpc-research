"""
ë…¼ë¬¸ì˜ ì‹¤í—˜ì„ ì¬í˜„í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì—ì„œ EHPC ì„±ëŠ¥ ê²€ì¦
"""

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import json
import logging
from datetime import datetime

from core.prompt_compressor import EHPCCompressor
from evaluation.benchmarks import EHPCBenchmark
from visualization.attention_viz import AttentionVisualizer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("experiment_log.txt"), logging.StreamHandler()],
)


def run_comprehensive_experiments():
    """í¬ê´„ì ì¸ EHPC ì‹¤í—˜ ì‹¤í–‰"""

    # ì‹¤í—˜ ì„¤ì •
    models_to_test = [
        "microsoft/DialoGPT-small",
        "microsoft/DialoGPT-medium",
        "gpt2",
    ]

    compression_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]

    all_results = {}
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    for model_name in models_to_test:
        logging.info(f"\n{'='*50}")
        logging.info(f"Testing model: {model_name}")
        logging.info(f"{'='*50}")

        try:
            # ì••ì¶•ê¸° ì´ˆê¸°í™”
            compressor = EHPCCompressor(model_name)
            compressor.initialize(max_layers=3, heads_per_layer=2)

            model_results = {
                "model_name": model_name,
                "evaluator_heads": [
                    {
                        "layer": head.layer,
                        "head": head.head,
                        "confidence": head.confidence_score,
                        "selectivity": head.selectivity_score,
                    }
                    for head in compressor.evaluator_heads
                ],
                "qa_results": {},
                "generation_examples": [],
            }

            # QA ë²¤ì¹˜ë§ˆí¬
            logging.info("Running QA benchmark...")
            benchmark = EHPCBenchmark(compressor)
            qa_results = benchmark.run_qa_benchmark(
                num_samples=50, compression_ratios=compression_ratios
            )
            model_results["qa_results"] = qa_results

            # ìƒì„± ì˜ˆì‹œ í…ŒìŠ¤íŠ¸
            logging.info("Testing generation examples...")
            test_prompts = [
                "Explain the concept of machine learning in simple terms.",
                "What are the key differences between supervised and unsupervised learning?",
                "Describe the attention mechanism in transformer models.",
                "How does gradient descent work in neural network training?",
                "What are the advantages and disadvantages of deep learning?",
            ]

            for i, prompt in enumerate(test_prompts):
                logging.info(f"Processing example {i+1}/5...")

                try:
                    result = compressor.compress_and_generate(
                        prompt,
                        compression_ratio=0.3,  # ê¸°ë³¸ 30% ì••ì¶•
                        max_new_tokens=100,
                    )

                    model_results["generation_examples"].append(
                        {
                            "prompt": prompt,
                            "compression_ratio": result["compression_ratio"],
                            "original_length": result["original_length"],
                            "compressed_length": result["compressed_length"],
                            "original_response": result["original_response"][:200]
                            + "...",
                            "compressed_response": result["compressed_response"][:200]
                            + "...",
                        }
                    )

                except Exception as e:
                    logging.warning(f"Failed to process example {i+1}: {e}")

            all_results[model_name] = model_results

            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            with open(
                f"results_{model_name.replace('/', '_')}_{timestamp}.json", "w"
            ) as f:
                json.dump(model_results, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logging.error(f"Failed to test model {model_name}: {e}")
            continue

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    final_results = {
        "timestamp": timestamp,
        "experiment_config": {
            "models": models_to_test,
            "compression_ratios": compression_ratios,
            "qa_samples": 50,
        },
        "results": all_results,
    }

    with open(f"comprehensive_results_{timestamp}.json", "w") as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print_experiment_summary(final_results)

    return final_results


def print_experiment_summary(results):
    """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print("EHPC ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")

    for model_name, model_results in results["results"].items():
        print(f"\nğŸ“Š ëª¨ë¸: {model_name}")
        print(f"   Evaluator Heads: {len(model_results['evaluator_heads'])}ê°œ")

        # QA ì„±ëŠ¥ ìš”ì•½
        qa_results = model_results["qa_results"]
        if qa_results:
            print("   QA ì„±ëŠ¥ (ì••ì¶•ë¥ ë³„):")
            for ratio, metrics in qa_results.items():
                f1_score = metrics.get("f1", 0)
                exact_match = metrics.get("exact_match", 0)
                print(f"     {ratio}: F1={f1_score:.3f}, EM={exact_match:.3f}")

        # ìƒì„± ì˜ˆì‹œ ìš”ì•½
        examples = model_results["generation_examples"]
        if examples:
            avg_compression = sum(ex["compression_ratio"] for ex in examples) / len(
                examples
            )
            print(f"   í‰ê·  ì••ì¶•ë¥ : {avg_compression:.1%}")

    print("\nğŸ’¾ ìƒì„¸ ê²°ê³¼ëŠ” JSON íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def create_visualization_report(results_file: str):
    """ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±"""
    with open(results_file, "r") as f:
        results = json.load(f)

    visualizer = AttentionVisualizer()

    # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±
    import matplotlib.pyplot as plt

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle("EHPC ì‹¤í—˜ ê²°ê³¼ ì¢…í•©", fontsize=16)

    # 1. ì••ì¶•ë¥ ë³„ F1 ì ìˆ˜
    ax1 = axes[0, 0]
    for model_name, model_results in results["results"].items():
        qa_results = model_results["qa_results"]
        if qa_results:
            ratios = [float(r) for r in qa_results.keys()]
            f1_scores = [qa_results[str(r)]["f1"] for r in ratios]
            ax1.plot(ratios, f1_scores, marker="o", label=model_name.split("/")[-1])

    ax1.set_title("ì••ì¶•ë¥ ë³„ F1 ì ìˆ˜")
    ax1.set_xlabel("ì••ì¶•ë¥ ")
    ax1.set_ylabel("F1 Score")
    ax1.legend()
    ax1.grid(True)

    # 2. Evaluator Heads ë¶„í¬
    ax2 = axes[0, 1]
    for model_name, model_results in results["results"].items():
        heads = model_results["evaluator_heads"]
        layers = [head["layer"] for head in heads]
        ax2.hist(layers, alpha=0.7, label=model_name.split("/")[-1], bins=range(4))

    ax2.set_title("Evaluator Heads ë ˆì´ì–´ ë¶„í¬")
    ax2.set_xlabel("ë ˆì´ì–´")
    ax2.set_ylabel("í—¤ë“œ ìˆ˜")
    ax2.legend()

    # 3. ì••ì¶• íš¨ìœ¨ì„±
    ax3 = axes[1, 0]
    model_names = []
    avg_compressions = []

    for model_name, model_results in results["results"].items():
        examples = model_results["generation_examples"]
        if examples:
            avg_compression = sum(ex["compression_ratio"] for ex in examples) / len(
                examples
            )
            model_names.append(model_name.split("/")[-1])
            avg_compressions.append(avg_compression)

    ax3.bar(
        model_names,
        avg_compressions,
        color=["skyblue", "lightcoral", "lightgreen"][: len(model_names)],
    )
    ax3.set_title("ëª¨ë¸ë³„ í‰ê·  ì••ì¶•ë¥ ")
    ax3.set_ylabel("ì••ì¶•ë¥ ")
    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)

    # 4. ì‹ ë¢°ë„ vs ì„ íƒì„±
    ax4 = axes[1, 1]
    for model_name, model_results in results["results"].items():
        heads = model_results["evaluator_heads"]
        confidences = [head["confidence"] for head in heads]
        selectivities = [head["selectivity"] for head in heads]
        ax4.scatter(
            selectivities, confidences, label=model_name.split("/")[-1], alpha=0.7
        )

    ax4.set_title("Evaluator Heads: ì‹ ë¢°ë„ vs ì„ íƒì„±")
    ax4.set_xlabel("ì„ íƒì„± ì ìˆ˜")
    ax4.set_ylabel("ì‹ ë¢°ë„ ì ìˆ˜")
    ax4.legend()
    ax4.grid(True)

    plt.tight_layout()
    plt.savefig(
        f'ehpc_experiment_report_{results["timestamp"]}.png',
        dpi=300,
        bbox_inches="tight",
    )
    plt.show()

    print(
        f"ğŸ“Š ì‹œê°í™” ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ehpc_experiment_report_{results['timestamp']}.png"
    )


if __name__ == "__main__":
    print("ğŸš€ EHPC ì¢…í•© ì‹¤í—˜ ì‹œì‘...")

    # ì‹¤í—˜ ì‹¤í–‰
    results = run_comprehensive_experiments()

    # ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±
    results_file = f"comprehensive_results_{results['timestamp']}.json"
    create_visualization_report(results_file)

    print("âœ… ëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
