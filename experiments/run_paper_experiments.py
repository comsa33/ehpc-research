"""
ë…¼ë¬¸ì˜ ì‹¤í—˜ì„ ì¬í˜„í•˜ëŠ” ì—…ê·¸ë ˆì´ë“œëœ ìŠ¤í¬ë¦½íŠ¸
ìµœì‹  ëª¨ë¸ë“¤ê³¼ í•œêµ­ì–´ ì§€ì›ì„ í¬í•¨í•œ ì¢…í•©ì ì¸ EHPC ì„±ëŠ¥ ê²€ì¦
"""

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import torch
from rich.console import Console
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)
from rich.table import Table

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from core.evaluator_heads import ModelConfig, get_model_info
from core.prompt_compressor import EHPCCompressor, create_compressor
from evaluation.benchmarks import EHPCBenchmark
from visualization.attention_viz import AttentionVisualizer

# Rich ì½˜ì†” ì„¤ì •
console = Console()


# ë¡œê¹… ì„¤ì •
def setup_logging(log_file: str):
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )


class EHPCExperimentRunner:
    """EHPC ì‹¤í—˜ ì‹¤í–‰ê¸° (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)"""

    def __init__(self, output_dir: str = "experiment_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ë¡œê¹… ì„¤ì •
        log_file = self.output_dir / f"experiment_{self.timestamp}.log"
        setup_logging(str(log_file))

        # ì‹¤í—˜ ì„¤ì •
        self.compression_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
        self.test_prompts = self._get_test_prompts()

        console.print(
            Panel.fit(
                f"ğŸ§ª EHPC ì‹¤í—˜ ì‹¤í–‰ê¸° v2.0\n"
                f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {self.output_dir}",
                title="ì‹¤í—˜ ì´ˆê¸°í™”",
                border_style="blue",
            )
        )

    def _get_test_prompts(self) -> Dict[str, List[str]]:
        """í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë‹¤êµ­ì–´ ì§€ì›)"""
        return {
            "english": [
                "Explain the concept of machine learning in simple terms.",
                "What are the key differences between supervised and unsupervised learning?",
                "Describe the attention mechanism in transformer models and its importance.",
                "How does gradient descent work in neural network training?",
                "What are the advantages and disadvantages of deep learning?",
                "Compare different optimization algorithms used in machine learning.",
                "Explain the concept of overfitting and how to prevent it.",
                "What is the difference between classification and regression tasks?",
            ],
            "korean": [
                "ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°œë…ì„ ê°„ë‹¨í•œ ìš©ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "ì§€ë„í•™ìŠµê³¼ ë¹„ì§€ë„í•™ìŠµì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ê·¸ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "ì‹ ê²½ë§ í›ˆë ¨ì—ì„œ ê²½ì‚¬í•˜ê°•ë²•ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
                "ë”¥ëŸ¬ë‹ì˜ ì¥ì ê³¼ ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê¸°ê³„í•™ìŠµì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµí•´ì£¼ì„¸ìš”.",
                "ê³¼ì í•©ì˜ ê°œë…ê³¼ ì´ë¥¼ ë°©ì§€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "ë¶„ë¥˜ ì‘ì—…ê³¼ íšŒê·€ ì‘ì—…ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            ],
            "technical": [
                "The transformer architecture revolutionized NLP through self-attention mechanisms, enabling parallel processing and capturing long-range dependencies in sequences.",
                "Large language models utilize billions of parameters to understand and generate human-like text across diverse domains and languages.",
                "Prompt engineering involves crafting input instructions to optimize model performance for specific tasks without fine-tuning.",
                "Attention heads in transformers specialize in different linguistic phenomena, from syntax parsing to semantic understanding.",
                "BERT's bidirectional encoder enables better context understanding compared to traditional left-to-right language models.",
            ],
        }

    def get_test_models(self, model_filter: Optional[str] = None) -> List[str]:
        """í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡ ìƒì„±"""
        all_models = {
            # Tier 1: ìµœê³  ì„±ëŠ¥ (ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ í•„ìš”)
            "tier1": [
                "meta-llama/Llama-3.2-3B-Instruct",
                "Qwen/Qwen2.5-3B-Instruct",
                "beomi/KoAlpaca-Polyglot-5.8B",
            ],
            # Tier 2: ê· í˜•í˜•
            "tier2": [
                "microsoft/Phi-3.5-mini-instruct",
                "google/gemma-2-2b",
                "Qwen/Qwen2.5-1.5B-Instruct",
            ],
            # Tier 3: ê²½ëŸ‰í˜• (ê¸°ë³¸ í…ŒìŠ¤íŠ¸ìš©)
            "tier3": ["microsoft/DialoGPT-medium", "google/gemma-2-2b"],
        }

        if model_filter:
            if model_filter in all_models:
                return all_models[model_filter]
            elif model_filter in ModelConfig.SUPPORTED_MODELS:
                return [model_filter]
            else:
                console.print(f"[red]ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_filter}[/red]")
                return []

        # í•˜ë“œì›¨ì–´ ê¸°ë°˜ ìë™ ì„ íƒ
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if gpu_memory >= 16:
                return all_models["tier1"]
            elif gpu_memory >= 8:
                return all_models["tier2"][:2] + [all_models["tier1"][1]]  # Qwen í¬í•¨
            else:
                return all_models["tier3"]
        else:
            # CPU í™˜ê²½
            return ["microsoft/DialoGPT-medium"]

    def run_comprehensive_experiments(
        self,
        models: Optional[List[str]] = None,
        include_qa: bool = True,
        include_generation: bool = True,
        include_compression: bool = True,
        num_qa_samples: int = 30,
    ) -> Dict:
        """í¬ê´„ì ì¸ EHPC ì‹¤í—˜ ì‹¤í–‰"""

        if models is None:
            models = self.get_test_models()

        console.print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ëª¨ë¸: {', '.join(models)}")

        all_results = {
            "experiment_info": {
                "timestamp": self.timestamp,
                "models_tested": models,
                "compression_ratios": self.compression_ratios,
                "qa_samples": num_qa_samples if include_qa else 0,
                "generation_samples": (
                    len(self.test_prompts["english"]) if include_generation else 0
                ),
                "hardware": self._get_hardware_info(),
            },
            "results": {},
        }

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console,
        ) as progress:

            overall_task = progress.add_task("ì „ì²´ ì‹¤í—˜ ì§„í–‰", total=len(models))

            for model_idx, model_name in enumerate(models):
                model_task = progress.add_task(
                    f"ëª¨ë¸ {model_name.split('/')[-1]} í…ŒìŠ¤íŠ¸", total=100
                )

                try:
                    console.print(f"\n{'='*80}")
                    console.print(
                        f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸: [bold blue]{model_name}[/bold blue]"
                    )
                    console.print(f"{'='*80}")

                    # ëª¨ë¸ ë¡œë”©
                    progress.update(
                        model_task, advance=10, description="ëª¨ë¸ ë¡œë”© ì¤‘..."
                    )
                    compressor = self._load_model_safely(model_name)

                    if compressor is None:
                        console.print(
                            f"[red]âŒ ëª¨ë¸ {model_name} ë¡œë”© ì‹¤íŒ¨, ìŠ¤í‚µ[/red]"
                        )
                        progress.update(overall_task, advance=1)
                        continue

                    model_results = {
                        "model_name": model_name,
                        "model_info": compressor.get_compression_stats(),
                        "experiments": {},
                    }

                    # 1. ì••ì¶• ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
                    if include_compression:
                        progress.update(
                            model_task, advance=20, description="ì••ì¶• ì„±ëŠ¥ í…ŒìŠ¤íŠ¸..."
                        )
                        compression_results = self._run_compression_tests(compressor)
                        model_results["experiments"][
                            "compression"
                        ] = compression_results

                    # 2. QA ë²¤ì¹˜ë§ˆí¬
                    if include_qa:
                        progress.update(
                            model_task, advance=30, description="QA ë²¤ì¹˜ë§ˆí¬..."
                        )
                        qa_results = self._run_qa_benchmark(compressor, num_qa_samples)
                        model_results["experiments"]["qa"] = qa_results

                    # 3. ìƒì„± í…ŒìŠ¤íŠ¸
                    if include_generation:
                        progress.update(
                            model_task, advance=30, description="ìƒì„± í…ŒìŠ¤íŠ¸..."
                        )
                        generation_results = self._run_generation_tests(compressor)
                        model_results["experiments"]["generation"] = generation_results

                    progress.update(model_task, advance=10, description="ê²°ê³¼ ì €ì¥...")
                    all_results["results"][model_name] = model_results

                    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                    self._save_intermediate_results(model_name, model_results)

                    console.print(f"âœ… ëª¨ë¸ {model_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

                except Exception as e:
                    console.print(f"[red]âŒ ëª¨ë¸ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}[/red]")
                    logging.error(f"ëª¨ë¸ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    continue

                finally:
                    progress.update(overall_task, advance=1)
                    progress.remove_task(model_task)

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    self._cleanup_memory()

        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_results_file = (
            self.output_dir / f"comprehensive_results_{self.timestamp}.json"
        )
        with open(final_results_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self._print_experiment_summary(all_results)

        # ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±
        self._create_visualization_report(final_results_file)

        return all_results

    def _load_model_safely(self, model_name: str) -> Optional[EHPCCompressor]:
        """ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©"""
        try:
            compressor = create_compressor(model_name, auto_initialize=False)

            # ì´ˆê¸°í™” ìˆ˜í–‰ (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ì¡°ì •)
            model_info = get_model_info(model_name)
            params = model_info.get("params", "2B")

            if "5B" in params or "8B" in params:
                max_layers, heads_per_layer = 2, 1  # í° ëª¨ë¸ì€ ì œí•œì ìœ¼ë¡œ
            elif "3B" in params:
                max_layers, heads_per_layer = 3, 2
            else:
                max_layers, heads_per_layer = 3, 2

            compressor.initialize(
                max_layers=max_layers, heads_per_layer=heads_per_layer
            )

            return compressor

        except Exception as e:
            logging.error(f"ëª¨ë¸ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def _run_compression_tests(self, compressor: EHPCCompressor) -> Dict:
        """ì••ì¶• ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        test_texts = []

        # ë‹¤ì–‘í•œ ì–¸ì–´ì˜ í…ìŠ¤íŠ¸ ì¤€ë¹„
        for lang_key, prompts in self.test_prompts.items():
            test_texts.extend(prompts[:3])  # ê° ì–¸ì–´ì—ì„œ 3ê°œì”©

        results = compressor.benchmark_compression(
            test_texts=test_texts, compression_ratios=[0.2, 0.3, 0.5, 0.7]
        )

        return results

    def _run_qa_benchmark(self, compressor: EHPCCompressor, num_samples: int) -> Dict:
        """QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        try:
            benchmark = EHPCBenchmark(compressor)
            results = benchmark.run_qa_benchmark(
                num_samples=num_samples, compression_ratios=[0.3, 0.5, 0.7]
            )
            return results
        except Exception as e:
            logging.warning(f"QA ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _run_generation_tests(self, compressor: EHPCCompressor) -> Dict:
        """ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        results = {"english_tests": [], "korean_tests": [], "technical_tests": []}

        # ê° ì–¸ì–´ë³„ í…ŒìŠ¤íŠ¸
        for lang_key, prompts in self.test_prompts.items():
            lang_results = []

            for i, prompt in enumerate(prompts[:3]):  # ê° ì–¸ì–´ì—ì„œ 3ê°œì”© í…ŒìŠ¤íŠ¸
                try:
                    result = compressor.compress_and_generate(
                        prompt,
                        compression_ratio=0.3,
                        max_new_tokens=80,
                        temperature=0.7,
                    )

                    lang_results.append(
                        {
                            "prompt_id": i,
                            "original_prompt": prompt[:100] + "...",  # ì²˜ìŒ 100ìë§Œ
                            "compression_ratio": result["compression_ratio"],
                            "original_length": result["original_length"],
                            "compressed_length": result["compressed_length"],
                            "generation_time_original": result.get(
                                "generation_time_original", 0
                            ),
                            "generation_time_compressed": result.get(
                                "generation_time_compressed", 0
                            ),
                            "speedup": result.get("generation_time_original", 0)
                            / max(result.get("generation_time_compressed", 1), 0.001),
                        }
                    )

                except Exception as e:
                    logging.warning(
                        f"ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì–¸ì–´: {lang_key}, í”„ë¡¬í”„íŠ¸ {i}): {e}"
                    )
                    continue

            results[f"{lang_key}_tests"] = lang_results

        return results

    def _get_hardware_info(self) -> Dict:
        """í•˜ë“œì›¨ì–´ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "python_version": sys.version,
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
        }

        if torch.cuda.is_available():
            info.update(
                {
                    "gpu_count": torch.cuda.device_count(),
                    "gpu_name": torch.cuda.get_device_name(0),
                    "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory
                    / (1024**3),
                }
            )

        return info

    def _save_intermediate_results(self, model_name: str, results: Dict):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        safe_model_name = model_name.replace("/", "_")
        intermediate_file = (
            self.output_dir / f"results_{safe_model_name}_{self.timestamp}.json"
        )

        with open(intermediate_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        import gc

        gc.collect()

    def _print_experiment_summary(self, results: Dict):
        """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        console.print("\n" + "=" * 80)
        console.print("ğŸ“Š EHPC ì‹¤í—˜ ê²°ê³¼ ìš”ì•½", style="bold blue")
        console.print("=" * 80)

        # ì „ì²´ í†µê³„
        experiment_info = results["experiment_info"]
        console.print(f"ğŸ• ì‹¤í—˜ ì‹œê°„: {experiment_info['timestamp']}")
        console.print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ëª¨ë¸: {len(experiment_info['models_tested'])}ê°œ")
        console.print(f"ğŸ“Š ì••ì¶• ë¹„ìœ¨: {len(experiment_info['compression_ratios'])}ê°œ")

        # ëª¨ë¸ë³„ ê²°ê³¼ í…Œì´ë¸”
        table = Table(title="ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
        table.add_column("ëª¨ë¸", style="cyan")
        table.add_column("íŒŒë¼ë¯¸í„°", style="magenta")
        table.add_column("Evaluator Heads", style="green")
        table.add_column("í‰ê·  ì••ì¶•ë¥ ", style="yellow")
        table.add_column("QA ì„±ëŠ¥", style="blue")
        table.add_column("ìƒì„± ì†ë„", style="red")

        for model_name, model_results in results["results"].items():
            model_short = model_name.split("/")[-1]

            # ê¸°ë³¸ ì •ë³´
            model_info = model_results.get("model_info", {}).get("model_info", {})
            params = model_info.get("parameters", "Unknown")
            heads_count = model_results.get("model_info", {}).get(
                "num_evaluator_heads", 0
            )

            # ì••ì¶• ì„±ëŠ¥
            compression_exp = model_results.get("experiments", {}).get(
                "compression", {}
            )
            avg_compression = "N/A"
            if compression_exp and "0.3" in compression_exp:
                avg_compression = (
                    f"{compression_exp['0.3'].get('avg_actual_ratio', 0):.1%}"
                )

            # QA ì„±ëŠ¥
            qa_exp = model_results.get("experiments", {}).get("qa", {})
            qa_performance = "N/A"
            if qa_exp and not qa_exp.get("error") and "0.3" in qa_exp:
                f1_score = qa_exp["0.3"].get("f1", 0)
                qa_performance = f"{f1_score:.3f}"

            # ìƒì„± ì†ë„
            gen_exp = model_results.get("experiments", {}).get("generation", {})
            gen_speed = "N/A"
            if gen_exp.get("english_tests"):
                avg_speedup = sum(
                    test.get("speedup", 1) for test in gen_exp["english_tests"]
                ) / len(gen_exp["english_tests"])
                gen_speed = f"{avg_speedup:.1f}x"

            table.add_row(
                model_short,
                str(params),
                str(heads_count),
                avg_compression,
                qa_performance,
                gen_speed,
            )

        console.print(table)

        # ì¶”ê°€ ì¸ì‚¬ì´íŠ¸
        console.print("\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_models = self._find_best_models(results["results"])
        for category, model_name in best_models.items():
            if model_name:
                console.print(f"â€¢ {category}: [bold]{model_name.split('/')[-1]}[/bold]")

    def _find_best_models(self, results: Dict) -> Dict[str, Optional[str]]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°"""
        best = {
            "ìµœê³  QA ì„±ëŠ¥": None,
            "ìµœê³  ì••ì¶• íš¨ìœ¨": None,
            "ìµœê³  ìƒì„± ì†ë„": None,
            "í•œêµ­ì–´ ìµœì ": None,
        }

        best_qa_score = 0
        best_compression_ratio = 0
        best_gen_speed = 0

        for model_name, model_results in results.items():
            experiments = model_results.get("experiments", {})

            # QA ì„±ëŠ¥
            qa_exp = experiments.get("qa", {})
            if qa_exp and not qa_exp.get("error") and "0.3" in qa_exp:
                f1_score = qa_exp["0.3"].get("f1", 0)
                if f1_score > best_qa_score:
                    best_qa_score = f1_score
                    best["ìµœê³  QA ì„±ëŠ¥"] = model_name

            # ì••ì¶• íš¨ìœ¨
            compression_exp = experiments.get("compression", {})
            if compression_exp and "0.3" in compression_exp:
                ratio = compression_exp["0.3"].get("avg_actual_ratio", 0)
                if ratio > best_compression_ratio:
                    best_compression_ratio = ratio
                    best["ìµœê³  ì••ì¶• íš¨ìœ¨"] = model_name

            # ìƒì„± ì†ë„
            gen_exp = experiments.get("generation", {})
            if gen_exp.get("english_tests"):
                avg_speedup = sum(
                    test.get("speedup", 1) for test in gen_exp["english_tests"]
                ) / len(gen_exp["english_tests"])
                if avg_speedup > best_gen_speed:
                    best_gen_speed = avg_speedup
                    best["ìµœê³  ìƒì„± ì†ë„"] = model_name

            # í•œêµ­ì–´ ìµœì  (KoAlpacaë‚˜ Qwen ì„ í˜¸)
            if "koalpaca" in model_name.lower() or "qwen" in model_name.lower():
                best["í•œêµ­ì–´ ìµœì "] = model_name

        return best

    def _create_visualization_report(self, results_file: Path):
        """ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            with open(results_file, "r", encoding="utf-8") as f:
                results = json.load(f)

            visualizer = AttentionVisualizer()

            # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±
            import matplotlib.pyplot as plt

            plt.style.use("default")

            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle("EHPC ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„", fontsize=16, fontweight="bold")

            # ë°ì´í„° ì¤€ë¹„
            models = []
            qa_scores = []
            compression_ratios = []
            gen_speeds = []
            head_counts = []

            for model_name, model_results in results["results"].items():
                models.append(model_name.split("/")[-1])

                # QA ì„±ëŠ¥
                qa_exp = model_results.get("experiments", {}).get("qa", {})
                if qa_exp and not qa_exp.get("error") and "0.3" in qa_exp:
                    qa_scores.append(qa_exp["0.3"].get("f1", 0))
                else:
                    qa_scores.append(0)

                # ì••ì¶•ë¥ 
                compression_exp = model_results.get("experiments", {}).get(
                    "compression", {}
                )
                if compression_exp and "0.3" in compression_exp:
                    compression_ratios.append(
                        compression_exp["0.3"].get("avg_actual_ratio", 0)
                    )
                else:
                    compression_ratios.append(0)

                # ìƒì„± ì†ë„
                gen_exp = model_results.get("experiments", {}).get("generation", {})
                if gen_exp.get("english_tests"):
                    avg_speedup = sum(
                        test.get("speedup", 1) for test in gen_exp["english_tests"]
                    ) / len(gen_exp["english_tests"])
                    gen_speeds.append(avg_speedup)
                else:
                    gen_speeds.append(1)

                # í—¤ë“œ ìˆ˜
                head_counts.append(
                    model_results.get("model_info", {}).get("num_evaluator_heads", 0)
                )

            # 1. QA ì„±ëŠ¥ ì°¨íŠ¸
            if qa_scores and any(qa_scores):
                axes[0, 0].bar(models, qa_scores, color="skyblue", alpha=0.7)
                axes[0, 0].set_title("QA ì„±ëŠ¥ (F1 Score)")
                axes[0, 0].set_ylabel("F1 Score")
                axes[0, 0].tick_params(axis="x", rotation=45)

            # 2. ì••ì¶•ë¥  ì°¨íŠ¸
            if compression_ratios and any(compression_ratios):
                axes[0, 1].bar(
                    models, compression_ratios, color="lightcoral", alpha=0.7
                )
                axes[0, 1].set_title("í‰ê·  ì••ì¶•ë¥ ")
                axes[0, 1].set_ylabel("ì••ì¶•ë¥ ")
                axes[0, 1].tick_params(axis="x", rotation=45)

            # 3. ìƒì„± ì†ë„ ì°¨íŠ¸
            if gen_speeds and any(s > 1 for s in gen_speeds):
                axes[1, 0].bar(models, gen_speeds, color="lightgreen", alpha=0.7)
                axes[1, 0].set_title("ìƒì„± ì†ë„ í–¥ìƒ")
                axes[1, 0].set_ylabel("ì†ë„ ë°°ìˆ˜")
                axes[1, 0].tick_params(axis="x", rotation=45)

            # 4. Evaluator Heads ìˆ˜
            if head_counts and any(head_counts):
                axes[1, 1].bar(models, head_counts, color="gold", alpha=0.7)
                axes[1, 1].set_title("Evaluator Heads ìˆ˜")
                axes[1, 1].set_ylabel("í—¤ë“œ ìˆ˜")
                axes[1, 1].tick_params(axis="x", rotation=45)

            plt.tight_layout()

            # íŒŒì¼ ì €ì¥
            report_file = (
                self.output_dir / f"ehpc_experiment_report_{self.timestamp}.png"
            )
            plt.savefig(report_file, dpi=300, bbox_inches="tight")
            plt.close()

            console.print(f"ğŸ“Š ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")

        except Exception as e:
            console.print(f"[red]âš ï¸ ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}[/red]")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="EHPC ì¢…í•© ì‹¤í—˜ ì‹¤í–‰ê¸°")
    parser.add_argument("--models", nargs="+", help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡")
    parser.add_argument(
        "--model-tier", choices=["tier1", "tier2", "tier3"], help="ëª¨ë¸ í‹°ì–´ ì„ íƒ"
    )
    parser.add_argument(
        "--output-dir", default="experiment_results", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--qa-samples", type=int, default=30, help="QA ë²¤ì¹˜ë§ˆí¬ ìƒ˜í”Œ ìˆ˜"
    )
    parser.add_argument("--skip-qa", action="store_true", help="QA ë²¤ì¹˜ë§ˆí¬ ìŠ¤í‚µ")
    parser.add_argument(
        "--skip-generation", action="store_true", help="ìƒì„± í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ"
    )
    parser.add_argument(
        "--skip-compression", action="store_true", help="ì••ì¶• í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ"
    )
    parser.add_argument(
        "--quick", action="store_true", help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ ìˆ˜ ê°ì†Œ)"
    )

    args = parser.parse_args()

    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„¤ì •
    if args.quick:
        args.qa_samples = 10
        console.print("[yellow]âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”[/yellow]")

    # ì‹¤í—˜ ì‹¤í–‰ê¸° ì´ˆê¸°í™”
    runner = EHPCExperimentRunner(args.output_dir)

    # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ê²°ì •
    test_models = None
    if args.models:
        test_models = args.models
    elif args.model_tier:
        test_models = runner.get_test_models(args.model_tier)

    # ì‹¤í—˜ ì‹¤í–‰
    try:
        results = runner.run_comprehensive_experiments(
            models=test_models,
            include_qa=not args.skip_qa,
            include_generation=not args.skip_generation,
            include_compression=not args.skip_compression,
            num_qa_samples=args.qa_samples,
        )

        console.print(
            Panel.fit(
                "âœ… ëª¨ë“  ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
                f"ğŸ“ ê²°ê³¼ëŠ” {runner.output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
                title="ì‹¤í—˜ ì™„ë£Œ",
                border_style="green",
            )
        )

    except KeyboardInterrupt:
        console.print("[red]âŒ ì‚¬ìš©ìì— ì˜í•´ ì‹¤í—˜ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.[/red]")

    except Exception as e:
        console.print(f"[red]âŒ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}[/red]")
        logging.error(f"ì‹¤í—˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
