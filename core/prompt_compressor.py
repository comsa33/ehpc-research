import logging
import re
from typing import Dict, List, Optional, Union

import numpy as np
import torch

from core.evaluator_heads import (
    CompressionResult,
    EvaluatorHeadFinder,
    EvaluatorHeadInfo,
    get_recommended_model,
    safe_tokenize,
)


class EHPCCompressor:
    """EHPC í”„ë¡¬í”„íŠ¸ ì••ì¶•ê¸° - ë…¼ë¬¸ì˜ ë©”ì¸ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)"""

    def __init__(self, model_name: Optional[str] = None, auto_initialize: bool = False):
        """
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ ìë™ ì„ íƒ)
            auto_initialize: Trueë©´ ìƒì„± ì‹œ ë°”ë¡œ ì´ˆê¸°í™”
        """
        if model_name is None:
            model_name = get_recommended_model()
            logging.info(f"ğŸ¯ ìë™ ì„ íƒëœ ëª¨ë¸: {model_name}")

        self.model_name = model_name
        self.head_finder = EvaluatorHeadFinder(model_name)
        self.evaluator_heads: Optional[List[EvaluatorHeadInfo]] = None
        self.is_initialized = False

        # ëª¨ë¸ ì •ë³´ ë¡œê¹…
        model_info = self.head_finder.get_model_info_dict()
        logging.info(f"ğŸ“š ë¡œë“œëœ ëª¨ë¸ ì •ë³´: {model_info}")

        if auto_initialize:
            self.initialize()

    def initialize(
        self,
        max_layers: int = 3,
        heads_per_layer: int = 2,
        force_reinitialize: bool = False,
    ) -> List[EvaluatorHeadInfo]:
        """
        ì‹œìŠ¤í…œ ì´ˆê¸°í™” - Evaluator Heads ì°¾ê¸°

        Args:
            max_layers: ê²€ì‚¬í•  ìµœëŒ€ ë ˆì´ì–´ ìˆ˜
            heads_per_layer: ë ˆì´ì–´ë‹¹ ì„ íƒí•  í—¤ë“œ ìˆ˜
            force_reinitialize: ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš°ì—ë„ ê°•ì œë¡œ ì¬ì´ˆê¸°í™”
        """
        if self.is_initialized and not force_reinitialize:
            logging.info("âš¡ ì´ë¯¸ ì´ˆê¸°í™”ë¨, ê¸°ì¡´ ê²°ê³¼ ì‚¬ìš©")
            return self.evaluator_heads

        logging.info("ğŸš€ EHPC ì••ì¶•ê¸° ì´ˆê¸°í™” ì‹œì‘...")

        # ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì ì‘ì  ì„¤ì •
        model_config = self.head_finder.model_config
        params = model_config.get("params", "1B")

        # ì‘ì€ ëª¨ë¸ì˜ ê²½ìš° ë” ë§ì€ ë ˆì´ì–´ ê²€ì‚¬
        if "2B" in params or "354M" in params:
            max_layers = min(max_layers, 4)
            heads_per_layer = max(heads_per_layer, 2)
        elif "3B" in params or "5B" in params:
            max_layers = min(max_layers, 3)
            heads_per_layer = max(heads_per_layer, 2)

        logging.info(
            f"ğŸ“Š ì„¤ì •: max_layers={max_layers}, heads_per_layer={heads_per_layer}"
        )

        try:
            self.evaluator_heads = self.head_finder.find_evaluator_heads(
                max_layers=max_layers, heads_per_layer=heads_per_layer
            )
            self.is_initialized = True

            logging.info(
                f"âœ… ì´ˆê¸°í™” ì™„ë£Œ: {len(self.evaluator_heads)}ê°œ Evaluator Heads ë°œê²¬"
            )
            return self.evaluator_heads

        except Exception as e:
            logging.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ë°±ì—… í—¤ë“œ ì‚¬ìš©
            self.evaluator_heads = [
                EvaluatorHeadInfo(
                    layer=0, head=0, selectivity_score=0.5, confidence_score=0.1
                )
            ]
            self.is_initialized = True
            logging.warning("âš ï¸ ë°±ì—… í—¤ë“œë¡œ ì´ˆê¸°í™”")
            return self.evaluator_heads

    def compress_prompt(
        self,
        text: str,
        compression_ratio: float = 0.3,
        preserve_special_tokens: bool = True,
        min_tokens: int = 5,
        max_tokens: Optional[int] = None,
    ) -> CompressionResult:
        """
        í”„ë¡¬í”„íŠ¸ ì••ì¶• ë©”ì¸ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)

        Args:
            text: ì••ì¶•í•  í…ìŠ¤íŠ¸
            compression_ratio: ìœ ì§€í•  í† í° ë¹„ìœ¨ (0.3 = 30% ìœ ì§€)
            preserve_special_tokens: íŠ¹ìˆ˜ í† í° ë³´ì¡´ ì—¬ë¶€
            min_tokens: ìµœì†Œ ìœ ì§€ í† í° ìˆ˜
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
        """
        if not self.is_initialized:
            logging.info("ğŸ”§ ìë™ ì´ˆê¸°í™” ìˆ˜í–‰...")
            self.initialize()

        # í† í°í™” ë° ê¸¸ì´ ì œí•œ
        model_max_length = self.head_finder.model_config.get("context_length", 2048)
        effective_max_length = min(model_max_length, max_tokens or 2048)

        inputs = safe_tokenize(
            self.head_finder.tokenizer,
            text,
            model_name=self.model_name,
            model_config=self.head_finder.model_config,
            max_length=effective_max_length,
        )

        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.head_finder.device) for k, v in inputs.items()}
        tokens = self.head_finder.tokenizer.convert_ids_to_tokens(
            inputs["input_ids"][0]
        )

        if len(tokens) == 0:
            raise ValueError("âŒ í† í°í™” ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        # Attention ê³„ì‚°
        with torch.no_grad():
            try:
                outputs = self.head_finder.model(**inputs)
                attentions = outputs.attentions
            except Exception as e:
                logging.error(f"âŒ ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                # í´ë°±: ê· ë“± ë¶„í¬ ì ìˆ˜ ì‚¬ìš©
                token_scores = np.ones(len(tokens)) / len(tokens)
                logging.warning("âš ï¸ í´ë°±: ê· ë“± ë¶„í¬ ì ìˆ˜ ì‚¬ìš©")
            else:
                # í† í° ì¤‘ìš”ë„ ê³„ì‚°
                token_scores = self._calculate_token_importance(attentions, tokens)

        # ì••ì¶•í•  í† í° ìˆ˜ ê²°ì •
        target_length = max(
            int(len(tokens) * compression_ratio),
            min_tokens,
            min(len(tokens), 3),  # ìµœì†Œí•œ 3ê°œëŠ” ìœ ì§€
        )

        if max_tokens and target_length > max_tokens:
            target_length = max_tokens

        # ì¤‘ìš”í•œ í† í°ë“¤ ì„ íƒ
        selected_indices = self._select_important_tokens(
            token_scores, target_length, tokens, preserve_special_tokens
        )

        # ì••ì¶•ëœ í† í°ë“¤
        compressed_tokens = [tokens[i] for i in selected_indices]
        actual_compression_ratio = len(compressed_tokens) / len(tokens)

        logging.info(
            f"ğŸ“Š ì••ì¶• ê²°ê³¼: {len(tokens)} â†’ {len(compressed_tokens)} í† í° "
            f"(ëª©í‘œ: {compression_ratio:.1%}, ì‹¤ì œ: {actual_compression_ratio:.1%})"
        )

        return CompressionResult(
            original_tokens=tokens,
            compressed_tokens=compressed_tokens,
            token_scores=token_scores,
            selected_indices=selected_indices,
            compression_ratio=actual_compression_ratio,
            evaluator_heads=self.evaluator_heads,
        )

    def _calculate_token_importance(
        self, attentions: tuple, tokens: List[str]
    ) -> np.ndarray:
        """Evaluator headsì˜ attentionì„ ë°”íƒ•ìœ¼ë¡œ í† í° ì¤‘ìš”ë„ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)"""
        num_tokens = len(tokens)
        importance_scores = np.zeros(num_tokens)

        if not self.evaluator_heads:
            logging.warning("âš ï¸ Evaluator headsê°€ ì—†ìŒ, ê· ë“± ë¶„í¬ ë°˜í™˜")
            return np.ones(num_tokens) / num_tokens

        total_weight = 0
        for head_info in self.evaluator_heads:
            layer_idx = head_info.layer
            head_idx = head_info.head

            if layer_idx < len(attentions):
                try:
                    # í•´ë‹¹ í—¤ë“œì˜ attention ê°€ì ¸ì˜¤ê¸°
                    attention = attentions[layer_idx][0, head_idx]  # [seq_len, seq_len]

                    # í¬ê¸° ê²€ì¦
                    if (
                        attention.size(0) != num_tokens
                        or attention.size(1) != num_tokens
                    ):
                        logging.debug(
                            f"Attention í¬ê¸° ë¶ˆì¼ì¹˜: {attention.shape} vs {num_tokens}"
                        )
                        continue

                    # ê°œì„ ëœ ì¤‘ìš”ë„ ê³„ì‚°
                    # 1. ê° í† í°ì´ ë°›ëŠ” attention (ë‹¤ë¥¸ í† í°ë“¤ë¡œë¶€í„°)
                    incoming_attention = attention.sum(dim=0).cpu().numpy()

                    # 2. ê° í† í°ì´ ì£¼ëŠ” attentionì˜ ë¶„ì‚° (ì„ íƒì„±)
                    outgoing_attention = attention.sum(dim=1).cpu().numpy()

                    # 3. Self-attention ê°€ì¤‘ì¹˜ (ëŒ€ê°ì„  ìš”ì†Œ)
                    self_attention = torch.diag(attention).cpu().numpy()

                    # 4. ê°€ì¤‘ ê²°í•© (ë” ì •êµí•œ ê³µì‹)
                    combined_score = (
                        0.5 * incoming_attention
                        + 0.3 * outgoing_attention
                        + 0.2 * self_attention
                    )

                    # í—¤ë“œì˜ ì‹ ë¢°ë„ë¡œ ê°€ì¤‘
                    head_weight = head_info.confidence_score
                    weighted_score = combined_score * head_weight

                    importance_scores += weighted_score
                    total_weight += head_weight

                except Exception as e:
                    logging.debug(f"í—¤ë“œ {layer_idx}-{head_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

        # ì •ê·œí™”
        if total_weight > 0:
            importance_scores = importance_scores / total_weight

        if importance_scores.max() > 0:
            importance_scores = importance_scores / importance_scores.max()
        else:
            # í´ë°±: ìœ„ì¹˜ ê¸°ë°˜ ì¤‘ìš”ë„ (ì‹œì‘ê³¼ ë ê°•ì¡°)
            importance_scores = self._get_positional_importance(num_tokens)
            logging.warning("âš ï¸ Attention ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨, ìœ„ì¹˜ ê¸°ë°˜ í´ë°± ì‚¬ìš©")

        return importance_scores

    def _get_positional_importance(self, num_tokens: int) -> np.ndarray:
        """ìœ„ì¹˜ ê¸°ë°˜ ì¤‘ìš”ë„ ì ìˆ˜ (í´ë°±ìš©)"""
        scores = np.ones(num_tokens)

        # ì‹œì‘ í† í°ë“¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
        start_boost = min(3, num_tokens // 4)
        scores[:start_boost] *= 1.5

        # ë í† í°ë“¤ì— ë†’ì€ ê°€ì¤‘ì¹˜
        end_boost = min(2, num_tokens // 6)
        if end_boost > 0:
            scores[-end_boost:] *= 1.3

        return scores / scores.sum()

    def _select_important_tokens(
        self,
        scores: np.ndarray,
        target_length: int,
        tokens: List[str],
        preserve_special_tokens: bool,
    ) -> List[int]:
        """ì¤‘ìš”ë„ ì ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ í† í° ì„ íƒ (ê°œì„ ëœ ë²„ì „)"""

        # íŠ¹ìˆ˜ í† í° ì‹ë³„ (í™•ì¥ëœ ë¦¬ìŠ¤íŠ¸)
        special_token_indices = set()
        if preserve_special_tokens:
            special_patterns = [
                r"^<[^>]*>$",  # <start>, <end> ë“±
                r"^\[[^\]]*\]$",  # [CLS], [SEP] ë“±
                r"^â–",  # SentencePiece prefix
            ]

            special_tokens = {
                "[CLS]",
                "[SEP]",
                "[PAD]",
                "[UNK]",
                "[MASK]",
                "<s>",
                "</s>",
                "<pad>",
                "<unk>",
                "<|endoftext|>",
                "Ä ",  # GPT-2 space token
            }

            for i, token in enumerate(tokens):
                # íŒ¨í„´ ë§¤ì¹­
                if any(re.match(pattern, token) for pattern in special_patterns):
                    special_token_indices.add(i)
                # ì§ì ‘ ë§¤ì¹­
                elif token in special_tokens:
                    special_token_indices.add(i)
                # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ í† í° ë³´ì¡´
                elif i == 0 or i == len(tokens) - 1:
                    special_token_indices.add(i)

        # íŠ¹ìˆ˜ í† í°ì€ í•­ìƒ í¬í•¨
        selected_indices = list(special_token_indices)
        remaining_slots = max(0, target_length - len(selected_indices))

        if remaining_slots > 0:
            # ì¼ë°˜ í† í°ë“¤ ì¤‘ì—ì„œ ì„ íƒ
            candidates = [
                (i, scores[i])
                for i in range(len(scores))
                if i not in special_token_indices
            ]

            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            candidates.sort(key=lambda x: x[1], reverse=True)

            # ìƒìœ„ í† í°ë“¤ ì„ íƒ
            for i, (token_idx, score) in enumerate(candidates[:remaining_slots]):
                selected_indices.append(token_idx)

        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        selected_indices.sort()

        # ì—°ì†ì„± í™•ë³´ë¥¼ ìœ„í•œ í›„ì²˜ë¦¬
        selected_indices = self._ensure_continuity(selected_indices, tokens)

        return selected_indices

    def _ensure_continuity(self, indices: List[int], tokens: List[str]) -> List[int]:
        """í…ìŠ¤íŠ¸ ì—°ì†ì„±ì„ ìœ„í•œ ì¸ë±ìŠ¤ ì¡°ì •"""
        if len(indices) <= 3:
            return indices

        # ë„ˆë¬´ í° ê°„ê²©ì´ ìˆëŠ” ê²½ìš° ì¤‘ê°„ í† í° ì¶”ê°€
        final_indices = []
        prev_idx = indices[0]
        final_indices.append(prev_idx)

        for curr_idx in indices[1:]:
            gap = curr_idx - prev_idx

            # ê°„ê²©ì´ ë„ˆë¬´ í¬ë©´ ì¤‘ê°„ì— í† í° í•˜ë‚˜ ì¶”ê°€
            if gap > 5 and len(final_indices) < len(indices) * 1.2:
                mid_idx = (prev_idx + curr_idx) // 2
                if mid_idx not in final_indices:
                    final_indices.append(mid_idx)

            final_indices.append(curr_idx)
            prev_idx = curr_idx

        return sorted(list(set(final_indices)))

    def tokens_to_text(self, tokens: List[str]) -> str:
        """í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê°œì„ ëœ ë²„ì „)"""
        if not tokens:
            return ""

        try:
            # í† í¬ë‚˜ì´ì €ì˜ ë³€í™˜ í•¨ìˆ˜ ì‚¬ìš©
            text = self.head_finder.tokenizer.convert_tokens_to_string(tokens)
        except Exception as e:
            logging.warning(f"í† í° ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}, ìˆ˜ë™ ë³€í™˜ ì‹œë„")
            # ìˆ˜ë™ ë³€í™˜ (í´ë°±)
            text = " ".join(tokens)
            # íŠ¹ìˆ˜ í† í° ì œê±°
            text = re.sub(r"<[^>]*>", "", text)
            text = re.sub(r"\[[^\]]*\]", "", text)

        # í›„ì²˜ë¦¬ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ë§Œë“¤ê¸°
        text = self._post_process_text(text)

        return text.strip()

    def _post_process_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬"""
        # ê¸°ë³¸ ì •ë¦¬
        text = re.sub(r"\s+", " ", text)  # ì¤‘ë³µ ê³µë°± ì œê±°
        text = re.sub(r"\s+([,.!?;:])", r"\1", text)  # êµ¬ë‘ì  ì• ê³µë°± ì œê±°
        text = re.sub(r"([,.!?;:])\s*([,.!?;:])", r"\1\2", text)  # ì¤‘ë³µ êµ¬ë‘ì  ì •ë¦¬

        # SentencePiece í† í° ì •ë¦¬
        text = re.sub(r"â–", " ", text)  # SentencePiece prefix ì œê±°
        text = re.sub(r"Ä ", " ", text)  # GPT-2 space token ì œê±°

        # ì¶”ê°€ ì •ë¦¬
        text = re.sub(r"\s+", " ", text)  # ë‹¤ì‹œ í•œ ë²ˆ ê³µë°± ì •ë¦¬

        return text

    def compress_and_generate(
        self,
        text: str,
        compression_ratio: float = 0.3,
        max_new_tokens: int = 100,
        temperature: float = 0.7,
        do_sample: bool = True,
    ) -> Dict[str, Union[str, float, int]]:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ì••ì¶•í•˜ê³  ê°™ì€ ëª¨ë¸ë¡œ ìƒì„±ê¹Œì§€ ìˆ˜í–‰ (ê°œì„ ëœ ë²„ì „)
        """
        # 1. ì›ë³¸ìœ¼ë¡œ ìƒì„±
        original_result = self._generate_response(
            text, max_new_tokens, temperature, do_sample
        )

        # 2. ì••ì¶• í›„ ìƒì„±
        compression_result = self.compress_prompt(text, compression_ratio)
        compressed_text = self.tokens_to_text(compression_result.compressed_tokens)

        compressed_result = self._generate_response(
            compressed_text, max_new_tokens, temperature, do_sample
        )

        # ê²°ê³¼ ì •ë¦¬
        return {
            "original_text": text,
            "compressed_text": compressed_text,
            "original_response": original_result["response"],
            "compressed_response": compressed_result["response"],
            "compression_ratio": compression_result.compression_ratio,
            "original_length": len(text.split()),
            "compressed_length": len(compressed_text.split()),
            "tokens_kept": len(compression_result.selected_indices),
            "tokens_total": len(compression_result.original_tokens),
            "original_tokens_count": original_result["input_tokens"],
            "compressed_tokens_count": compressed_result["input_tokens"],
            "generation_time_original": original_result["generation_time"],
            "generation_time_compressed": compressed_result["generation_time"],
        }

    def _generate_response(
        self, text: str, max_new_tokens: int, temperature: float, do_sample: bool
    ) -> Dict:
        """ì‘ë‹µ ìƒì„± (ì‹œê°„ ì¸¡ì • í¬í•¨) - KoAlpaca í˜¸í™˜ì„± ê°œì„ """
        import time

        start_time = time.time()

        try:
            inputs = safe_tokenize(
                self.head_finder.tokenizer,
                text,
                model_name=self.model_name,
                model_config=self.head_finder.model_config,
                max_length=1024,
            )

            inputs = {k: v.to(self.head_finder.device) for k, v in inputs.items()}
            input_tokens = inputs["input_ids"].size(1)

            with torch.no_grad():
                # KoAlpaca ëª¨ë¸ íŠ¹í™” ìƒì„± ì„¤ì •
                generation_kwargs = {
                    "max_new_tokens": max_new_tokens,
                    "temperature": temperature,
                    "do_sample": do_sample,
                    "pad_token_id": self.head_finder.tokenizer.eos_token_id,
                    "eos_token_id": self.head_finder.tokenizer.eos_token_id,
                    "repetition_penalty": 1.1,
                }

                # KoAlpaca ëª¨ë¸ì¸ ê²½ìš° ì¶”ê°€ ì„¤ì •
                if "koalpaca" in self.model_name.lower():
                    generation_kwargs.update(
                        {
                            "top_p": 0.95,
                            "top_k": 50,
                            "no_repeat_ngram_size": 3,
                        }
                    )

                outputs = self.head_finder.model.generate(**inputs, **generation_kwargs)

            response = self.head_finder.tokenizer.decode(
                outputs[0], skip_special_tokens=True
            )

            generation_time = time.time() - start_time

            return {
                "response": response,
                "input_tokens": input_tokens,
                "generation_time": generation_time,
            }

        except Exception as e:
            logging.error(f"ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "response": f"[ìƒì„± ì˜¤ë¥˜: {str(e)}]",
                "input_tokens": 0,
                "generation_time": time.time() - start_time,
            }

    def get_compression_stats(self) -> Dict:
        """ì••ì¶•ê¸° í†µê³„ ì •ë³´"""
        model_info = self.head_finder.get_model_info_dict()

        return {
            "model_name": self.model_name,
            "model_info": model_info,
            "is_initialized": self.is_initialized,
            "num_evaluator_heads": (
                len(self.evaluator_heads) if self.evaluator_heads else 0
            ),
            "evaluator_heads": (
                [
                    {
                        "layer": head.layer,
                        "head": head.head,
                        "confidence": head.confidence_score,
                        "selectivity": head.selectivity_score,
                    }
                    for head in (self.evaluator_heads or [])
                ]
                if self.evaluator_heads
                else []
            ),
        }

    def benchmark_compression(
        self, test_texts: List[str], compression_ratios: List[float] = None
    ) -> Dict:
        """ì••ì¶• ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        if compression_ratios is None:
            compression_ratios = [0.2, 0.3, 0.5, 0.7]

        results = {}

        for ratio in compression_ratios:
            ratio_results = {
                "compression_ratio": ratio,
                "results": [],
                "avg_actual_ratio": 0,
                "avg_compression_time": 0,
            }

            total_actual_ratio = 0
            total_time = 0

            for text in test_texts:
                try:
                    import time

                    start_time = time.time()

                    result = self.compress_prompt(text, ratio)
                    compression_time = time.time() - start_time

                    ratio_results["results"].append(
                        {
                            "original_length": len(result.original_tokens),
                            "compressed_length": len(result.compressed_tokens),
                            "actual_ratio": result.compression_ratio,
                            "compression_time": compression_time,
                        }
                    )

                    total_actual_ratio += result.compression_ratio
                    total_time += compression_time

                except Exception as e:
                    logging.warning(f"ë²¤ì¹˜ë§ˆí¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            if ratio_results["results"]:
                ratio_results["avg_actual_ratio"] = total_actual_ratio / len(
                    ratio_results["results"]
                )
                ratio_results["avg_compression_time"] = total_time / len(
                    ratio_results["results"]
                )

            results[str(ratio)] = ratio_results

        return results


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_compressor(
    model_name: Optional[str] = None, auto_initialize: bool = True
) -> EHPCCompressor:
    """ê°„í¸í•œ ì••ì¶•ê¸° ìƒì„± í•¨ìˆ˜"""
    return EHPCCompressor(model_name, auto_initialize)


def quick_compress(
    text: str, ratio: float = 0.3, model_name: Optional[str] = None
) -> str:
    """ë¹ ë¥¸ ì••ì¶• í•¨ìˆ˜"""
    compressor = create_compressor(model_name, auto_initialize=True)
    result = compressor.compress_prompt(text, ratio)
    return compressor.tokens_to_text(result.compressed_tokens)


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_compression():
    """ì••ì¶• ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    test_text = """
    ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ì´ë©°, íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ë°œì „ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.
    ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë“¤ì€ í…ìŠ¤íŠ¸ ì´í•´ì™€ ìƒì„±ì—ì„œ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìœ¼ë©°,
    ì´ëŠ” ë‹¤ì–‘í•œ ì‹¤ë¬´ ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ í”„ë¡¬í”„íŠ¸ ì••ì¶• ê¸°ìˆ ì€
    ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œ ë°œìƒí•˜ëŠ” ê³„ì‚° ë¹„ìš©ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤.
    """

    try:
        print("ğŸ§ª ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        compressor = create_compressor(auto_initialize=True)
        print(f"âœ… ì••ì¶•ê¸° ìƒì„± ì™„ë£Œ: {compressor.model_name}")

        result = compressor.compress_prompt(test_text, compression_ratio=0.3)
        compressed_text = compressor.tokens_to_text(result.compressed_tokens)

        print("ğŸ“Š ì••ì¶• ê²°ê³¼:")
        print(f"   ì›ë³¸ í† í°: {len(result.original_tokens)}")
        print(f"   ì••ì¶• í† í°: {len(result.compressed_tokens)}")
        print(f"   ì••ì¶•ë¥ : {result.compression_ratio:.1%}")
        print(f"   ì••ì¶•ëœ í…ìŠ¤íŠ¸: {compressed_text[:100]}...")

        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_compression()
