import logging
import os
from dataclasses import dataclass
from typing import Dict, List, Optional

import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig


@dataclass
class EvaluatorHeadInfo:
    """Evaluator Head ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„°í´ë˜ìŠ¤"""

    layer: int
    head: int
    selectivity_score: float  # ì„ íƒì„± ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì§‘ì¤‘ì )
    confidence_score: float  # ì‹ ë¢°ë„ ì ìˆ˜


@dataclass
class CompressionResult:
    """ì••ì¶• ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„°í´ë˜ìŠ¤"""

    original_tokens: List[str]
    compressed_tokens: List[str]
    token_scores: np.ndarray
    selected_indices: List[int]
    compression_ratio: float
    evaluator_heads: List[EvaluatorHeadInfo]


class ModelConfig:
    """ëª¨ë¸ë³„ ì„¤ì • ì •ë³´"""

    SUPPORTED_MODELS = {
        # ì¶”ì²œ ëª¨ë¸ë“¤ (ì„±ëŠ¥ ìˆœ)
        "google/gemma-2-2b": {
            "params": "2B",
            "min_memory_gb": 4,
            "quantize": True,
            "context_length": 8192,
            "pros": ["ë¹ ë¥¸ ì†ë„", "ìµœì‹  ì•„í‚¤í…ì²˜", "ì•ˆì •ì "],
            "cons": ["ì‘ì€ ëª¨ë¸ í¬ê¸°"],
            "korean_support": 3,  # 1-5 ì ìˆ˜
        },
        "Qwen/Qwen2.5-3B-Instruct": {
            "params": "3B",
            "min_memory_gb": 6,
            "quantize": True,
            "context_length": 32768,
            "pros": ["í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜", "íš¨ìœ¨ì ", "ìµœì‹  ê¸°ìˆ "],
            "cons": ["ì¤‘êµ­ íšŒì‚¬ ëª¨ë¸"],
            "korean_support": 5,
        },
        "meta-llama/Llama-3.2-3B-Instruct": {
            "params": "3B",
            "min_memory_gb": 8,
            "quantize": True,
            "context_length": 131072,
            "pros": ["ìµœì‹  ì•„í‚¤í…ì²˜", "ê¸´ ì»¨í…ìŠ¤íŠ¸", "Meta ê³µì‹"],
            "cons": ["ë¼ì´ì„¼ìŠ¤ ì œì•½"],
            "korean_support": 3,
        },
        "microsoft/Phi-3.5-mini-instruct": {
            "params": "3.8B",
            "min_memory_gb": 6,
            "quantize": True,
            "context_length": 128000,
            "pros": ["Microsoft ìµœì‹ ", "íš¨ìœ¨ì ", "ê¸´ ì»¨í…ìŠ¤íŠ¸"],
            "cons": ["ìƒëŒ€ì ìœ¼ë¡œ ìƒˆë¡œì›€"],
            "korean_support": 3,
        },
        # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë“¤
        "beomi/KoAlpaca-Polyglot-5.8B": {
            "params": "5.8B",
            "min_memory_gb": 12,
            "quantize": True,
            "context_length": 2048,
            "pros": ["í•œêµ­ì–´ ìµœê³  ì„±ëŠ¥", "í•™ìˆ  ì—°êµ¬ìš©"],
            "cons": ["í° ë©”ëª¨ë¦¬ ì‚¬ìš©"],
            "korean_support": 5,
        },
        # ê¸°ì¡´ í˜¸í™˜ì„± ëª¨ë¸
        "microsoft/DialoGPT-medium": {
            "params": "354M",
            "min_memory_gb": 2,
            "quantize": False,
            "context_length": 1024,
            "pros": ["ê°€ë²¼ì›€", "ê¸°ì¡´ í˜¸í™˜ì„±"],
            "cons": ["êµ¬ì‹ ì•„í‚¤í…ì²˜", "ì„±ëŠ¥ ì œí•œ"],
            "korean_support": 1,
        },
    }

    @classmethod
    def get_recommended_model(cls) -> str:
        """í•˜ë“œì›¨ì–´ í™˜ê²½ì— ë§ëŠ” ëª¨ë¸ ì¶”ì²œ"""
        if torch.cuda.is_available():
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            logging.info(f"ğŸ® GPU ë©”ëª¨ë¦¬: {gpu_memory_gb:.1f}GB")

            if gpu_memory_gb >= 16:
                return "beomi/KoAlpaca-Polyglot-5.8B"  # í•œêµ­ì–´ ìµœê³  ì„±ëŠ¥
            elif gpu_memory_gb >= 12:
                return "meta-llama/Llama-3.2-3B-Instruct"  # ê³ ì„±ëŠ¥
            elif gpu_memory_gb >= 8:
                return "Qwen/Qwen2.5-3B-Instruct"  # ê· í˜•
            else:
                return "google/gemma-2-2b"  # ê²½ëŸ‰

        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            # Mac M1/M2/M3
            import psutil

            total_memory_gb = psutil.virtual_memory().total / (1024**3)
            logging.info(f"ğŸ Mac ë©”ëª¨ë¦¬: {total_memory_gb:.1f}GB")

            if total_memory_gb >= 32:
                return "Qwen/Qwen2.5-3B-Instruct"
            else:
                return "google/gemma-2-2b"
        else:
            # CPU í™˜ê²½
            logging.info("ğŸ’» CPU í™˜ê²½ ê°ì§€")
            return "microsoft/DialoGPT-medium"


class EvaluatorHeadFinder:
    """ë…¼ë¬¸ì˜ í•µì‹¬ - Evaluator Headsë¥¼ ì°¾ëŠ” í´ë˜ìŠ¤ (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)"""

    def __init__(self, model_name: Optional[str] = None):
        # ëª¨ë¸ ìë™ ì„ íƒ
        if model_name is None:
            model_name = ModelConfig.get_recommended_model()
            logging.info(f"ğŸ¯ ìë™ ì„ íƒëœ ëª¨ë¸: {model_name}")

        self.model_name = model_name
        self.model_config = ModelConfig.SUPPORTED_MODELS.get(model_name, {})
        self.device = self._get_optimal_device()

        logging.info(f"ğŸ“š ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
        logging.info(f"âš™ï¸ ì„¤ì •: {self.model_config}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self._load_tokenizer()

        # ëª¨ë¸ ë¡œë“œ
        self._load_model()

        self.model.eval()
        logging.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self._get_model_stats()}")

    def _get_hf_token(self) -> Optional[str]:
        """Hugging Face í† í° ê°€ì ¸ì˜¤ê¸°"""

        # 1. í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í° ì°¾ê¸°
        token = os.getenv("HUGGINGFACE_TOKEN") or os.getenv("HF_TOKEN")

        if token:
            logging.info("ğŸ”‘ í™˜ê²½ ë³€ìˆ˜ì—ì„œ HF í† í° ë°œê²¬")
            return token

        # 2. .env íŒŒì¼ì—ì„œ í† í° ì°¾ê¸°
        try:
            from pathlib import Path

            env_file = Path(".env")
            if env_file.exists():
                with open(env_file, "r") as f:
                    for line in f:
                        if line.startswith("HUGGINGFACE_TOKEN=") or line.startswith(
                            "HF_TOKEN="
                        ):
                            token = line.split("=", 1)[1].strip().strip("\"'")
                            logging.info("ğŸ”‘ .env íŒŒì¼ì—ì„œ HF í† í° ë°œê²¬")
                            return token
        except Exception as e:
            logging.debug(f".env íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

        # 3. Hugging Face CLI ìºì‹œì—ì„œ í† í° ì°¾ê¸°
        try:
            from huggingface_hub import HfFolder

            cached_token = HfFolder.get_token()
            if cached_token:
                logging.info("ğŸ”‘ HF CLI ìºì‹œì—ì„œ í† í° ë°œê²¬")
                return cached_token
        except Exception as e:
            logging.debug(f"HF CLI í† í° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

        # 4. í† í°ì´ ì—†ìœ¼ë©´ ë¡œê·¸ì¸ ì•ˆë‚´
        logging.warning("âš ï¸ Hugging Face í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logging.info("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ë¡œê·¸ì¸í•˜ì„¸ìš”:")
        logging.info("1. í™˜ê²½ ë³€ìˆ˜: export HUGGINGFACE_TOKEN='your_token'")
        logging.info("2. .env íŒŒì¼: echo 'HUGGINGFACE_TOKEN=your_token' > .env")
        logging.info("3. CLI ë¡œê·¸ì¸: huggingface-cli login")

        return None

    def _handle_gated_model_fallback(self, original_model: str) -> str:
        """Gated ëª¨ë¸ ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ í´ë°± ëª¨ë¸ ì„ íƒ"""
        fallback_models = {
            "google/gemma-2-2b": "microsoft/DialoGPT-medium",
            "google/gemma-2-2b-it": "microsoft/DialoGPT-medium",
            "meta-llama/Llama-3.2-3B-Instruct": "microsoft/Phi-3.5-mini-instruct",
            "meta-llama/Llama-3.2-8B-Instruct": "Qwen/Qwen2.5-3B-Instruct",
            "beomi/KoAlpaca-Polyglot-5.8B": "Qwen/Qwen2.5-3B-Instruct",
        }

        fallback = fallback_models.get(original_model, "microsoft/DialoGPT-medium")
        logging.info(f"ğŸ”„ Gated ëª¨ë¸ {original_model} â†’ í´ë°± ëª¨ë¸ {fallback}")
        return fallback

    def _get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ"""
        if torch.cuda.is_available():
            device = f"cuda:{torch.cuda.current_device()}"
            gpu_name = torch.cuda.get_device_name(0)
            logging.info(f"ğŸ® CUDA ë””ë°”ì´ìŠ¤: {gpu_name}")
            return device
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            logging.info("ğŸ Apple MPS ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
            return "mps"
        else:
            logging.info("ğŸ’» CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
            return "cpu"

    def _load_tokenizer(self):
        """í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            # Hugging Face ì¸ì¦ ì²˜ë¦¬
            token = self._get_hf_token()

            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True,
                token=token,  # ì¸ì¦ í† í° ì¶”ê°€
            )

            # íŒ¨ë“œ í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                if self.tokenizer.eos_token:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                else:
                    self.tokenizer.add_special_tokens({"pad_token": "[PAD]"})

            logging.info(f"ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: vocab_size={len(self.tokenizer)}")

        except Exception as e:
            logging.error(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _load_model(self):
        """ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©"""
        try:
            # Hugging Face ì¸ì¦ ì²˜ë¦¬
            token = self._get_hf_token()

            model_kwargs = {
                "output_attentions": True,
                "trust_remote_code": True,
                "attn_implementation": "eager",  # ì•ˆì •ì„± ìš°ì„ 
                "token": token,  # ì¸ì¦ í† í° ì¶”ê°€
            }

            # ì–‘ìí™” ì„¤ì • (CUDAì—ì„œë§Œ)
            if (
                self.model_config.get("quantize", False)
                and "cuda" in self.device
                and self._is_quantization_available()
            ):

                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16,
                    bnb_8bit_use_double_quant=True,
                )
                model_kwargs["quantization_config"] = quantization_config
                model_kwargs["torch_dtype"] = torch.float16
                logging.info("ğŸ”§ 8bit ì–‘ìí™” ì ìš©")

            elif "mps" in self.device:
                model_kwargs["torch_dtype"] = torch.float16
                logging.info("ğŸ MPSìš© float16 ì„¤ì •")
            else:
                model_kwargs["torch_dtype"] = torch.float32
                logging.info("ğŸ’» CPUìš© float32 ì„¤ì •")

            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name, **model_kwargs
            )

            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (ì–‘ìí™”ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
            if "quantization_config" not in model_kwargs:
                self.model = self.model.to(self.device)

        except Exception as e:
            logging.warning(f"âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, í´ë°± ì‹œë„: {e}")

            # Gated ëª¨ë¸ ì ‘ê·¼ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
            if "gated repo" in str(e).lower() or "access to model" in str(e).lower():
                fallback_model = self._handle_gated_model_fallback(self.model_name)
                logging.info(f"ğŸ”„ í´ë°± ëª¨ë¸ë¡œ ì¬ì‹œë„: {fallback_model}")

                # í´ë°± ëª¨ë¸ë¡œ ì¬ê·€ í˜¸ì¶œ
                original_model = self.model_name
                self.model_name = fallback_model
                self.model_config = ModelConfig.SUPPORTED_MODELS.get(fallback_model, {})

                try:
                    self._load_tokenizer()  # í† í¬ë‚˜ì´ì €ë„ ë‹¤ì‹œ ë¡œë“œ
                    self._load_model()  # ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ
                    logging.info(f"âœ… í´ë°± ëª¨ë¸ {fallback_model} ë¡œë“œ ì„±ê³µ")
                    return
                except Exception as fallback_error:
                    logging.error(f"âŒ í´ë°± ëª¨ë¸ë„ ì‹¤íŒ¨: {fallback_error}")
                    # ì›ë˜ ëª¨ë¸ëª… ë³µêµ¬ í›„ ì¼ë°˜ í´ë°±ìœ¼ë¡œ ë„˜ì–´ê°
                    self.model_name = original_model

            self._load_fallback_model()

    def _is_quantization_available(self) -> bool:
        """ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import bitsandbytes

            return True
        except ImportError:
            logging.warning("âš ï¸ bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ì–‘ìí™” ë¹„í™œì„±í™”")
            return False

    def _load_fallback_model(self):
        """í´ë°± ëª¨ë¸ ë¡œë”©"""
        fallback_models = ["google/gemma-2-2b", "microsoft/DialoGPT-medium", "gpt2"]

        for fallback in fallback_models:
            try:
                logging.info(f"ğŸ”„ í´ë°± ëª¨ë¸ ì‹œë„: {fallback}")

                self.model = AutoModelForCausalLM.from_pretrained(
                    fallback,
                    output_attentions=True,
                    torch_dtype=torch.float32,
                    attn_implementation="eager",
                )

                # í† í¬ë‚˜ì´ì €ë„ ë‹¤ì‹œ ë¡œë“œ
                self.tokenizer = AutoTokenizer.from_pretrained(fallback)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token

                self.model = self.model.to(self.device)
                self.model_name = fallback
                self.model_config = ModelConfig.SUPPORTED_MODELS.get(fallback, {})

                logging.info(f"âœ… í´ë°± ëª¨ë¸ ë¡œë”© ì„±ê³µ: {fallback}")
                return

            except Exception as e:
                logging.warning(f"âŒ í´ë°± ëª¨ë¸ {fallback} ì‹¤íŒ¨: {e}")
                continue

        raise RuntimeError("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹œë„ ì‹¤íŒ¨")

    def _get_model_stats(self) -> str:
        """ëª¨ë¸ í†µê³„ ì •ë³´"""
        try:
            param_count = sum(p.numel() for p in self.model.parameters()) / 1e6
            device_info = str(next(self.model.parameters()).device)
            dtype_info = str(next(self.model.parameters()).dtype)

            memory_usage = ""
            if "cuda" in device_info:
                memory_gb = torch.cuda.memory_allocated() / (1024**3)
                memory_usage = f", {memory_gb:.1f}GB ì‚¬ìš©"

            return f"{param_count:.1f}M params on {device_info} ({dtype_info}){memory_usage}"
        except:
            return "í†µê³„ ì •ë³´ ì—†ìŒ"

    def create_needle_haystack_data(
        self, num_samples: int = 20
    ) -> List[Dict[str, str]]:
        """
        ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ Needle-in-a-Haystack í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (í•œêµ­ì–´ ì¶”ê°€)
        """
        # ì˜ì–´ templates
        haystack_templates_en = [
            "The weather was beautiful today with clear blue skies and gentle winds.",
            "Technology continues to advance at an unprecedented pace in various fields.",
            "Literature has always been a mirror reflecting human experiences and emotions.",
            "Mathematics provides the foundation for understanding patterns in nature.",
            "History teaches us valuable lessons about human civilization and progress.",
            "Music has the power to transcend cultural boundaries and unite people.",
            "Science explores the mysteries of the universe through systematic observation.",
            "Philosophy questions the fundamental nature of existence and knowledge.",
            "Art expresses human creativity and imagination in countless beautiful forms.",
            "Education empowers individuals to reach their full potential in life.",
        ]

        # í•œêµ­ì–´ templates (í•œêµ­ì–´ ì§€ì› ëª¨ë¸ìš©)
        haystack_templates_ko = [
            "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ë§‘ì€ í•˜ëŠ˜ê³¼ ë¶€ë“œëŸ¬ìš´ ë°”ëŒìœ¼ë¡œ ë§¤ìš° ì•„ë¦„ë‹¤ì› ìŠµë‹ˆë‹¤.",
            "ê¸°ìˆ ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì „ë¡€ ì—†ëŠ” ì†ë„ë¡œ ê³„ì† ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ë¬¸í•™ì€ í•­ìƒ ì¸ê°„ì˜ ê²½í—˜ê³¼ ê°ì •ì„ ë°˜ì˜í•˜ëŠ” ê±°ìš¸ ì—­í• ì„ í•´ì™”ìŠµë‹ˆë‹¤.",
            "ìˆ˜í•™ì€ ìì—°ì˜ íŒ¨í„´ì„ ì´í•´í•˜ëŠ” ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            "ì—­ì‚¬ëŠ” ì¸ê°„ ë¬¸ëª…ê³¼ ë°œì „ì— ëŒ€í•œ ê·€ì¤‘í•œ êµí›ˆì„ ê°€ë¥´ì³ ì¤ë‹ˆë‹¤.",
            "ìŒì•…ì€ ë¬¸í™”ì  ê²½ê³„ë¥¼ ë„˜ë‚˜ë“¤ë©° ì‚¬ëŒë“¤ì„ í•˜ë‚˜ë¡œ ë¬¶ëŠ” í˜ì´ ìˆìŠµë‹ˆë‹¤.",
            "ê³¼í•™ì€ ì²´ê³„ì ì¸ ê´€ì°°ì„ í†µí•´ ìš°ì£¼ì˜ ì‹ ë¹„ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤.",
            "ì² í•™ì€ ì¡´ì¬ì™€ ì§€ì‹ì˜ ê·¼ë³¸ì  ë³¸ì§ˆì— ëŒ€í•´ ì§ˆë¬¸í•©ë‹ˆë‹¤.",
        ]

        needles_en = [
            "The secret code is ALPHA-7842.",
            "The hidden treasure is buried under the old oak tree.",
            "The password for the system is 'BlueSky2024'.",
            "The meeting will be held at room 314 on Friday.",
            "The special ingredient is three tablespoons of vanilla extract.",
        ]

        needles_ko = [
            "ë¹„ë°€ ì½”ë“œëŠ” ALPHA-7842ì…ë‹ˆë‹¤.",
            "ìˆ¨ê²¨ì§„ ë³´ë¬¼ì€ ì˜¤ë˜ëœ ì°¸ë‚˜ë¬´ ì•„ë˜ì— ë¬»í˜€ ìˆìŠµë‹ˆë‹¤.",
            "ì‹œìŠ¤í…œ ë¹„ë°€ë²ˆí˜¸ëŠ” 'BlueSky2024'ì…ë‹ˆë‹¤.",
            "íšŒì˜ëŠ” ê¸ˆìš”ì¼ 314í˜¸ì‹¤ì—ì„œ ì—´ë¦´ ì˜ˆì •ì…ë‹ˆë‹¤.",
            "íŠ¹ë³„í•œ ì¬ë£ŒëŠ” ë°”ë‹ë¼ ì¶”ì¶œë¬¼ 3í°ìˆ ì…ë‹ˆë‹¤.",
        ]

        # í•œêµ­ì–´ ì§€ì› ì ìˆ˜ì— ë”°ë¼ ì–¸ì–´ ì„ íƒ
        korean_support = self.model_config.get("korean_support", 1)
        use_korean = korean_support >= 4

        haystack_templates = (
            haystack_templates_ko if use_korean else haystack_templates_en
        )
        needles = needles_ko if use_korean else needles_en

        logging.info(f"ğŸŒ í…ŒìŠ¤íŠ¸ ì–¸ì–´: {'í•œêµ­ì–´' if use_korean else 'ì˜ì–´'}")

        test_data = []
        for i in range(num_samples):
            # ë¬´ì‘ìœ„ë¡œ haystack ë¬¸ì¥ë“¤ ì„ íƒ
            haystack_sentences = np.random.choice(
                haystack_templates, size=np.random.randint(8, 15), replace=True
            )
            needle = np.random.choice(needles)

            # needleì„ ì¤‘ê°„ ìœ„ì¹˜ì— ì‚½ì…
            sentences = list(haystack_sentences)
            insert_pos = len(sentences) // 2
            sentences.insert(insert_pos, needle)

            full_text = " ".join(sentences)
            test_data.append(
                {
                    "text": full_text,
                    "needle": needle,
                    "answer": needle.split()[-1],  # ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ë‹µìœ¼ë¡œ ì‚¬ìš©
                    "language": "korean" if use_korean else "english",
                }
            )

        return test_data

    def evaluate_head_performance(
        self, layer_idx: int, head_idx: int, test_data: List[Dict[str, str]]
    ) -> float:
        """íŠ¹ì • í—¤ë“œê°€ Needle-in-a-Haystack ì‘ì—…ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰í•˜ëŠ”ì§€ í‰ê°€ (ì•ˆì •ì„± ê°•í™”)"""
        correct_predictions = 0
        valid_samples = 0

        for data in test_data:
            try:
                text = data["text"]
                needle = data["needle"]

                # í† í°í™” ë° ê¸¸ì´ ì œí•œ
                max_length = min(self.model_config.get("context_length", 2048), 1024)
                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=max_length,
                    padding=False,
                )

                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                # ë¹ˆ ì…ë ¥ ì²´í¬
                if inputs["input_ids"].size(1) == 0:
                    continue

                tokens = self.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

                with torch.no_grad():
                    outputs = self.model(**inputs)
                    attentions = outputs.attentions

                # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì²´í¬
                if layer_idx >= len(attentions) or head_idx >= attentions[
                    layer_idx
                ].size(1):
                    continue

                attention = attentions[layer_idx][0, head_idx]  # [seq_len, seq_len]

                if attention.size(0) == 0 or attention.size(1) == 0:
                    continue

                # needle í† í°ë“¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸° (ê°œì„ ëœ ë°©ë²•)
                needle_tokens = self.tokenizer.tokenize(needle)
                needle_indices = []

                # ë” ì •í™•í•œ needle ë§¤ì¹­
                needle_text_lower = needle.lower()
                for i, token in enumerate(tokens):
                    if i < attention.size(0):
                        token_text = (
                            self.tokenizer.convert_tokens_to_string([token])
                            .lower()
                            .strip()
                        )
                        if any(
                            needle_word in token_text
                            for needle_word in needle_text_lower.split()
                        ):
                            needle_indices.append(i)

                if needle_indices and len(tokens) > 0:
                    # ë§ˆì§€ë§‰ í† í°ì˜ attention íŒ¨í„´ ë¶„ì„
                    last_token_idx = min(len(tokens) - 1, attention.size(0) - 1)
                    if last_token_idx >= 0:
                        last_token_attention = attention[last_token_idx]

                        # ìœ íš¨í•œ needle ì¸ë±ìŠ¤ë“¤ë§Œ ì‚¬ìš©
                        valid_needle_indices = [
                            idx
                            for idx in needle_indices
                            if idx < last_token_attention.size(0)
                        ]

                        if valid_needle_indices:
                            needle_attention_sum = sum(
                                last_token_attention[idx]
                                for idx in valid_needle_indices
                            )

                            total_attention = last_token_attention.sum()
                            if total_attention > 0:
                                attention_ratio = needle_attention_sum / total_attention
                                # ì„ê³„ê°’ì„ ëª¨ë¸ì— ë”°ë¼ ì¡°ì •
                                threshold = (
                                    0.15
                                    if self.model_config.get("params", "").endswith("B")
                                    else 0.1
                                )

                                if attention_ratio > threshold:
                                    correct_predictions += 1

                valid_samples += 1

            except (RuntimeError, IndexError, KeyError) as e:
                logging.debug(f"ìƒ˜í”Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ìŠ¤í‚µ): {e}")
                continue

        return correct_predictions / valid_samples if valid_samples > 0 else 0.0

    def find_evaluator_heads(
        self, max_layers: int = 3, heads_per_layer: int = 2
    ) -> List[EvaluatorHeadInfo]:
        """Evaluator Headsë¥¼ ì°¾ëŠ” ë©”ì¸ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)"""
        logging.info("ğŸ” Needle-in-a-Haystack í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        test_data = self.create_needle_haystack_data(num_samples=30)  # ìƒ˜í”Œ ìˆ˜ ì¦ê°€

        logging.info("ğŸ§  Attention heads í‰ê°€ ì¤‘...")
        head_scores = []

        # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        try:
            config = self.model.config
            num_layers = min(max_layers, getattr(config, "num_hidden_layers", 12))
            num_heads = getattr(config, "num_attention_heads", 12)

            logging.info(
                f"ğŸ“Š ê²€ì‚¬ ë²”ìœ„: {num_layers}ê°œ ë ˆì´ì–´, ë ˆì´ì–´ë‹¹ {num_heads}ê°œ í—¤ë“œ"
            )

        except Exception as e:
            logging.warning(f"ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}. ê¸°ë³¸ê°’ ì‚¬ìš©.")
            num_layers = min(max_layers, 6)
            num_heads = 8

        # ì§„í–‰ ìƒí™© ì¶”ì 
        total_heads = num_layers * num_heads
        current_head = 0

        for layer_idx in range(num_layers):
            layer_heads = []

            for head_idx in range(num_heads):
                current_head += 1
                progress = (current_head / total_heads) * 100

                if current_head % 5 == 0:  # 5ê°œë§ˆë‹¤ ë¡œê·¸
                    logging.info(
                        f"ì§„í–‰ë¥ : {progress:.1f}% ({current_head}/{total_heads})"
                    )

                try:
                    # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
                    performance_score = self.evaluate_head_performance(
                        layer_idx, head_idx, test_data
                    )

                    # ì„ íƒì„± ì ìˆ˜ ê³„ì‚°
                    selectivity_score = self._calculate_head_selectivity(
                        layer_idx, head_idx, test_data[:5]  # ìƒ˜í”Œë§ìœ¼ë¡œ ì†ë„ í–¥ìƒ
                    )

                    head_info = EvaluatorHeadInfo(
                        layer=layer_idx,
                        head=head_idx,
                        selectivity_score=selectivity_score,
                        confidence_score=performance_score,
                    )

                    layer_heads.append(head_info)

                except Exception as e:
                    logging.debug(f"í—¤ë“œ {layer_idx}-{head_idx} í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            # ë ˆì´ì–´ë³„ ìƒìœ„ í—¤ë“œë“¤ë§Œ ì„ íƒ
            if layer_heads:
                layer_heads.sort(
                    key=lambda x: x.confidence_score + (1.0 - x.selectivity_score),
                    reverse=True,
                )
                head_scores.extend(layer_heads[:heads_per_layer])

        if not head_scores:
            # ë°±ì—…: ê¸°ë³¸ í—¤ë“œ ì„ íƒ
            logging.warning("âš ï¸ ìœ íš¨í•œ í—¤ë“œë¥¼ ì°¾ì§€ ëª»í•¨, ê¸°ë³¸ í—¤ë“œ ì‚¬ìš©")
            default_heads = [
                EvaluatorHeadInfo(
                    layer=0, head=0, selectivity_score=0.5, confidence_score=0.1
                ),
                EvaluatorHeadInfo(
                    layer=1, head=0, selectivity_score=0.5, confidence_score=0.1
                ),
            ]
            return default_heads

        # ìµœì¢… ì„ íƒ: ì „ì²´ ì¤‘ ìƒìœ„ í—¤ë“œë“¤
        head_scores.sort(
            key=lambda x: x.confidence_score + (1.0 - x.selectivity_score), reverse=True
        )

        # ìµœëŒ€ max_layers * heads_per_layer ê°œ ì„ íƒ
        max_selected = max_layers * heads_per_layer
        selected_heads = head_scores[:max_selected]

        logging.info(f"âœ… {len(selected_heads)}ê°œ Evaluator Heads ì„ íƒ ì™„ë£Œ:")
        for head in selected_heads:
            logging.info(
                f"  ğŸ“ Layer {head.layer}, Head {head.head}: "
                f"ì‹ ë¢°ë„={head.confidence_score:.3f}, ì„ íƒì„±={head.selectivity_score:.3f}"
            )

        return selected_heads

    def _calculate_head_selectivity(
        self, layer_idx: int, head_idx: int, test_data: List[Dict[str, str]]
    ) -> float:
        """í—¤ë“œì˜ ì„ íƒì„± ì ìˆ˜ ê³„ì‚° (ì•ˆì •ì„± ê°•í™”)"""
        total_entropy = 0
        valid_samples = 0

        for data in test_data:
            try:
                text = data["text"]
                max_length = min(self.model_config.get("context_length", 2048), 512)
                inputs = self.tokenizer(
                    text, return_tensors="pt", truncation=True, max_length=max_length
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                if inputs["input_ids"].size(1) == 0:
                    continue

                with torch.no_grad():
                    outputs = self.model(**inputs)
                    attentions = outputs.attentions

                if layer_idx < len(attentions) and head_idx < attentions[
                    layer_idx
                ].size(1):

                    attention = attentions[layer_idx][0, head_idx]

                    if attention.size(0) > 0 and attention.size(1) > 0:
                        # Attentionì„ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
                        attention_probs = torch.softmax(attention, dim=-1)

                        # NaN/ë¬´í•œê°’ ì²´í¬
                        if torch.isfinite(attention_probs).all():
                            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (í´ë¨í•‘ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´)
                            log_probs = torch.log(
                                torch.clamp(attention_probs, min=1e-12)
                            )
                            entropy = -torch.sum(attention_probs * log_probs, dim=-1)

                            if torch.isfinite(entropy).all():
                                total_entropy += entropy.mean().item()
                                valid_samples += 1

            except (RuntimeError, IndexError, KeyError) as e:
                logging.debug(f"ì„ íƒì„± ê³„ì‚° ì¤‘ ì˜¤ë¥˜ (ìŠ¤í‚µ): {e}")
                continue

        return total_entropy / valid_samples if valid_samples > 0 else 1.0

    def get_model_info_dict(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        try:
            config = self.model.config
            return {
                "model_name": self.model_name,
                "parameters": self.model_config.get("params", "Unknown"),
                "device": str(self.device),
                "num_layers": getattr(config, "num_hidden_layers", "Unknown"),
                "num_heads": getattr(config, "num_attention_heads", "Unknown"),
                "context_length": self.model_config.get("context_length", "Unknown"),
                "korean_support": self.model_config.get("korean_support", 1),
                "pros": self.model_config.get("pros", []),
                "quantized": self.model_config.get("quantize", False),
                "memory_usage": self._get_memory_usage(),
            }
        except Exception as e:
            return {"error": str(e)}

    def _get_memory_usage(self) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        try:
            if "cuda" in str(self.device):
                allocated = torch.cuda.memory_allocated() / (1024**3)
                cached = torch.cuda.memory_reserved() / (1024**3)
                return f"GPU: {allocated:.1f}GB allocated, {cached:.1f}GB cached"
            else:
                return "Non-CUDA device"
        except:
            return "Unknown"


# í¸ì˜ í•¨ìˆ˜ë“¤
def get_recommended_model() -> str:
    """ì‚¬ìš©ì í™˜ê²½ì— ë§ëŠ” ì¶”ì²œ ëª¨ë¸ ë°˜í™˜"""
    return ModelConfig.get_recommended_model()


def list_supported_models() -> List[str]:
    """ì§€ì›ë˜ëŠ” ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    return list(ModelConfig.SUPPORTED_MODELS.keys())


def get_model_info(model_name: str) -> Dict:
    """íŠ¹ì • ëª¨ë¸ì˜ ì •ë³´ ë°˜í™˜"""
    return ModelConfig.SUPPORTED_MODELS.get(model_name, {})


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_model_loading(model_name: Optional[str] = None):
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    try:
        if model_name is None:
            model_name = get_recommended_model()

        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë¸: {model_name}")

        finder = EvaluatorHeadFinder(model_name)
        print("âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ!")

        info = finder.get_model_info_dict()
        print(f"ğŸ“Š ëª¨ë¸ ì •ë³´: {info}")

        # ê°„ë‹¨í•œ í—¤ë“œ ì°¾ê¸° í…ŒìŠ¤íŠ¸
        print("ğŸ” Evaluator Heads ì°¾ê¸° í…ŒìŠ¤íŠ¸...")
        heads = finder.find_evaluator_heads(max_layers=2, heads_per_layer=1)
        print(f"âœ… {len(heads)}ê°œ í—¤ë“œ ë°œê²¬!")

        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    # ì§€ì› ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    print("ğŸ¯ ì§€ì›ë˜ëŠ” ëª¨ë¸ë“¤:")
    for model in list_supported_models():
        info = get_model_info(model)
        print(
            f"  - {model}: {info.get('params', 'Unknown')} ({info.get('korean_support', 1)}/5 í•œêµ­ì–´)"
        )

    print(f"\nğŸš€ ì¶”ì²œ ëª¨ë¸: {get_recommended_model()}")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("\nğŸ§ª ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    test_model_loading()
