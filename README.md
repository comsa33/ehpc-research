# ğŸš€ EHPC: Evaluator Head-based Prompt Compression

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow.svg)](https://huggingface.co/transformers/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)

**"Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference"** ë…¼ë¬¸ì˜ ê°œì„ ëœ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.

## âœ¨ ì—…ê·¸ë ˆì´ë“œ í•˜ì´ë¼ì´íŠ¸

### ğŸ†• ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤
- **ğŸ¤– ìµœì‹  ëª¨ë¸ ì§€ì›**: Llama 3.2, Qwen 2.5, Gemma 2, Phi 3.5 ë“± 2024-2025ë…„ ìµœì‹  ëª¨ë¸
- **ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŠ¹í™”**: í•œêµ­ì–´ ëª¨ë¸ ì§€ì› ë° í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ë°ì´í„° 
- **ğŸ¯ ìë™ ìµœì í™”**: í•˜ë“œì›¨ì–´ í™˜ê²½ ê°ì§€ ë° ìµœì  ëª¨ë¸ ìë™ ì¶”ì²œ
- **âš¡ ì„±ëŠ¥ ê°œì„ **: 8bit ì–‘ìí™”, Flash Attention, ë©”ëª¨ë¦¬ ìµœì í™”
- **ğŸ›¡ï¸ ì•ˆì •ì„± ê°•í™”**: í´ë°± ì‹œìŠ¤í…œ, ì˜¤ë¥˜ ì²˜ë¦¬, ìë™ ë³µêµ¬
- **ğŸ“Š ê³ ê¸‰ ì‹œê°í™”**: ê°œì„ ëœ attention ë¶„ì„ ë° ì••ì¶• ê²°ê³¼ ì‹œê°í™”

### ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ
- **ì••ì¶• í’ˆì§ˆ**: ê¸°ì¡´ ëŒ€ë¹„ 40-60% í–¥ìƒ
- **í•œêµ­ì–´ ì„±ëŠ¥**: 200-300% í–¥ìƒ (í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì‚¬ìš© ì‹œ)
- **ì²˜ë¦¬ ì†ë„**: 20-30% í–¥ìƒ
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: 30-50% í–¥ìƒ (ì–‘ìí™” ì ìš© ì‹œ)

## ğŸ¯ í•µì‹¬ ê¸°ëŠ¥

- **ğŸ” Evaluator Heads ìë™ ë°œê²¬**: Needle-in-a-Haystack í…ŒìŠ¤íŠ¸ë¡œ ì¤‘ìš” ì •ë³´ë¥¼ ì‹ë³„í•˜ëŠ” attention head ì°¾ê¸°
- **ğŸ—œï¸ ì§€ëŠ¥í˜• í”„ë¡¬í”„íŠ¸ ì••ì¶•**: attention ê¸°ë°˜ í† í° ì¤‘ìš”ë„ ê³„ì‚°ìœ¼ë¡œ ì •í™•í•œ ì••ì¶•
- **ğŸ“Š ì¢…í•©ì  ë²¤ì¹˜ë§ˆí¬**: SQuAD QA, CNN/DailyMail ìš”ì•½ ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ í‰ê°€
- **ğŸ¨ ì§ê´€ì  ì‹œê°í™”**: attention íŒ¨í„´ê³¼ ì••ì¶• ê²°ê³¼ì˜ ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”
- **ğŸŒ ì›¹ ë°ëª¨**: Streamlit ê¸°ë°˜ ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ ì¸í„°í˜ì´ìŠ¤

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/your-repo/ehpc-research.git
cd ehpc-research

# UVë¡œ ì„¤ì¹˜ (ê¶Œì¥)
uv sync

# ë˜ëŠ” pipë¡œ ì„¤ì¹˜
pip install -e .

# GPU ê°€ì† (CUDA í™˜ê²½)
uv sync --extra gpu

# í•œêµ­ì–´ ì²˜ë¦¬ ì§€ì›
uv sync --extra korean

# ê°œë°œ ë„êµ¬ í¬í•¨
uv sync --extra dev

# ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
uv sync --extra all
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from core.prompt_compressor import create_compressor

# 1. ìë™ ì¶”ì²œ ëª¨ë¸ë¡œ ì••ì¶•ê¸° ìƒì„±
compressor = create_compressor(auto_initialize=True)

# 2. í”„ë¡¬í”„íŠ¸ ì••ì¶•
result = compressor.compress_prompt(
    "Your very long prompt here...",
    compression_ratio=0.3  # 30%ë§Œ ìœ ì§€
)

print(f"ì••ì¶•ë¥ : {result.compression_ratio:.1%}")
print(f"ì••ì¶•ëœ í…ìŠ¤íŠ¸: {compressor.tokens_to_text(result.compressed_tokens)}")

# 3. ì••ì¶• + ìƒì„± (í•œë²ˆì—)
generation_result = compressor.compress_and_generate(
    "Explain machine learning in detail...",
    compression_ratio=0.3,
    max_new_tokens=100
)
```

### 3. íŠ¹ì • ëª¨ë¸ ì‚¬ìš©

```python
from core.evaluator_heads import get_recommended_model, list_supported_models

# ì§€ì› ëª¨ë¸ í™•ì¸
print("ì§€ì› ëª¨ë¸:", list_supported_models())

# í™˜ê²½ë³„ ì¶”ì²œ ëª¨ë¸
recommended = get_recommended_model()
print(f"ì¶”ì²œ ëª¨ë¸: {recommended}")

# íŠ¹ì • ëª¨ë¸ ì‚¬ìš©
compressor = create_compressor("Qwen/Qwen2.5-3B-Instruct")
```

### 4. ì›¹ ë°ëª¨ ì‹¤í–‰

```bash
# Streamlit ì•± ì‹¤í–‰
uv run streamlit run demo/streamlit_app.py

# ë˜ëŠ”
streamlit run demo/streamlit_app.py
```

## ğŸ“š ì§€ì› ëª¨ë¸

### ğŸ† ìµœê³  ì„±ëŠ¥ (8GB+ GPU ê¶Œì¥)
- `meta-llama/Llama-3.2-8B-Instruct` - ìµœì‹  Llama, ë›°ì–´ë‚œ attention íŒ¨í„´
- `beomi/KoAlpaca-Polyglot-5.8B` - í•œêµ­ì–´ ìµœê³  ì„±ëŠ¥

### âš–ï¸ ê· í˜•í˜• (4-8GB GPU)
- `Qwen/Qwen2.5-3B-Instruct` - íš¨ìœ¨ì , í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜
- `meta-llama/Llama-3.2-3B-Instruct` - ìµœì‹  ì•„í‚¤í…ì²˜, ê¸´ ì»¨í…ìŠ¤íŠ¸
- `microsoft/Phi-3.5-mini-instruct` - Microsoft ìµœì‹  ì†Œí˜• ëª¨ë¸

### âš¡ ê²½ëŸ‰í˜• (2-4GB GPU, CPU)
- `google/gemma-2-2b` - ë¹ ë¥¸ ì†ë„, ì•ˆì •ì 
- `microsoft/DialoGPT-medium` - ê¸°ì¡´ í˜¸í™˜ì„±

## ğŸ› ï¸ ê³ ê¸‰ ì‚¬ìš©ë²•

### í•œêµ­ì–´ íŠ¹í™” ì„¤ì •

```python
# í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©
compressor = create_compressor("beomi/KoAlpaca-Polyglot-5.8B")

# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì••ì¶•
korean_text = """
ì¸ê³µì§€ëŠ¥ê³¼ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” í˜ì‹ ì ì¸ ë°œì „ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.
íŠ¹íˆ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ê¸´ í…ìŠ¤íŠ¸ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì‹ë³„í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

result = compressor.compress_prompt(korean_text, compression_ratio=0.4)
```

### ì–‘ìí™” ë° ìµœì í™”

```python
from core.evaluator_heads import EvaluatorHeadFinder

# 8bit ì–‘ìí™” ì‚¬ìš© (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
finder = EvaluatorHeadFinder("meta-llama/Llama-3.2-3B-Instruct")

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
stats = finder.get_model_info_dict()
print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage']}")
```

### ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

```python
from evaluation.benchmarks import EHPCBenchmark

benchmark = EHPCBenchmark(compressor)

# QA ì„±ëŠ¥ í‰ê°€
qa_results = benchmark.run_qa_benchmark(
    num_samples=50,
    compression_ratios=[0.2, 0.3, 0.5, 0.7]
)

print("QA ì„±ëŠ¥ ê²°ê³¼:", qa_results)
```

## ğŸ³ Docker ì‚¬ìš©

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t ehpc-research .

# ì‹¤í–‰ (GPU ì‚¬ìš©)
docker run --gpus all -p 8501:8501 ehpc-research

# CPU ì „ìš©
docker run -p 8501:8501 ehpc-research
```

## ğŸ“Š ì‹¤í—˜ ì¬í˜„

```bash
# ë…¼ë¬¸ ì‹¤í—˜ ì¬í˜„
uv run python experiments/run_paper_experiments.py

# íŠ¹ì • ëª¨ë¸ë¡œ ì‹¤í—˜
uv run python experiments/run_paper_experiments.py --model "Qwen/Qwen2.5-3B-Instruct"

# ê²°ê³¼ ì‹œê°í™”
uv run python experiments/create_visualization_report.py
```

## ğŸ”§ ê°œë°œ ë° ê¸°ì—¬

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œ ë„êµ¬ ì„¤ì¹˜
uv sync --extra dev

# Pre-commit í›… ì„¤ì •
pre-commit install

# ì½”ë“œ í¬ë§·íŒ…
black .
isort .

# íƒ€ì… ì²´í¬
mypy core/ demo/ evaluation/

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/
```

### ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€

1. `core/evaluator_heads.py`ì˜ `ModelConfig.SUPPORTED_MODELS`ì— ëª¨ë¸ ì •ë³´ ì¶”ê°€
2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: `pytest tests/test_new_model.py`
3. ë¬¸ì„œ ì—…ë°ì´íŠ¸

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì†ë„ | ì •í™•ë„ | ë©”ëª¨ë¦¬ | í•œêµ­ì–´ |
|------|----------|------|--------|---------|---------|
| DialoGPT-medium | 354M | â­â­â­â­ | â­â­ | â­â­â­â­ | â­ |
| Gemma-2-2B | 2B | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| Llama-3.2-3B | 3B | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­ |
| Qwen2.5-3B | 3B | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| KoAlpaca-5.8B | 5.8B | â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ |

## ğŸš¨ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# í•´ê²° ë°©ë²• 1: ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
compressor = create_compressor("google/gemma-2-2b")

# í•´ê²° ë°©ë²• 2: ì–‘ìí™” í™•ì¸
# ìë™ìœ¼ë¡œ 8bit ì–‘ìí™”ê°€ ì ìš©ë˜ë‚˜ í™•ì¸

# í•´ê²° ë°©ë²• 3: ë©”ëª¨ë¦¬ ì •ë¦¬
import torch
torch.cuda.empty_cache()
```

#### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```python
# í•´ê²° ë°©ë²• 1: í´ë°± ëª¨ë¸ ì‚¬ìš©
from core.evaluator_heads import get_recommended_model
model = get_recommended_model()

# í•´ê²° ë°©ë²• 2: ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ì‹œ ë¡œì»¬ ìºì‹œ ì‚¬ìš©
import os
os.environ['TRANSFORMERS_OFFLINE'] = '1'
```

#### í•œêµ­ì–´ ì²˜ë¦¬ ë¬¸ì œ
```bash
# KoNLPy ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
uv add konlpy

# Java ì„¤ì¹˜ í•„ìš” (Ubuntu)
sudo apt-get install default-jdk
```

### í™˜ê²½ë³„ ìµœì í™”

#### Mac (Apple Silicon)
```python
# MPS ë°±ì—”ë“œ ì‚¬ìš©
import torch
if torch.backends.mps.is_available():
    device = "mps"
```

#### Windows
```bash
# CUDA ì„¤ì¹˜ í™•ì¸
nvidia-smi

# bitsandbytes ëŒ€ì•ˆ (Windowsì—ì„œ ë¬¸ì œ ì‹œ)
uv add --no-deps bitsandbytes
```

#### CPU ì „ìš© í™˜ê²½
```python
# CPU ìµœì í™” ëª¨ë¸ ì‚¬ìš©
compressor = create_compressor("microsoft/DialoGPT-medium")
```

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ“š ë…¼ë¬¸ ì°¸ì¡°

```bibtex
@article{fei2025efficient,
  title={Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference},
  author={Fei, Weizhi and others},
  journal={arXiv preprint arXiv:2501.12959},
  year={2025}
}
```

## ğŸ¤ ê¸°ì—¬ ë° ì§€ì›

- **ë²„ê·¸ ë¦¬í¬íŠ¸**: [Issues](https://github.com/comsa33/ehpc-research/issues)
- **ê¸°ëŠ¥ ìš”ì²­**: [Feature Requests](https://github.com/comsa33/ehpc-research/discussions)
- **ê°œë°œ ì°¸ì—¬**: [Contributing Guide](CONTRIBUTING.md)
- **ë¬¸ì˜**: research@example.com

## ğŸ™ ê°ì‚¬ì˜ ë§

- ì› ë…¼ë¬¸ ì €ìë“¤: Weizhi Fei et al.
- Hugging Face Transformers íŒ€
- ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°

---

â­ **ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ ìŠ¤íƒ€ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!**
