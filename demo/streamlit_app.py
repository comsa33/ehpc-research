import gc
import os
import sys
import time
import traceback

import streamlit as st
import torch

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

try:
    from core.evaluator_heads import (
        ModelConfig,
        get_model_info,
        get_recommended_model,
        list_supported_models,
        test_model_loading,
    )
    from core.prompt_compressor import EHPCCompressor, create_compressor
    from evaluation.benchmarks import EHPCBenchmark
    from visualization.attention_viz import AttentionVisualizer
except ImportError as e:
    st.error(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    st.error(
        "í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”: `streamlit run demo/streamlit_app.py`"
    )
    st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="EHPC Demo - ì—…ê·¸ë ˆì´ë“œ",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded",
)

# CSS ìŠ¤íƒ€ì¼
st.markdown(
    """
<style>
    .main-header {
        text-align: center;
        padding: 1rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f0f2f6;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
    }
    .success-box {
        background: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
    .warning-box {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
    .error-box {
        background: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
</style>
""",
    unsafe_allow_html=True,
)


# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜
def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


# ì•ˆì „í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì•ˆì „ ì´ˆê¸°í™”"""
    defaults = {
        "current_model": None,
        "initialized": False,
        "evaluator_heads": [],
        "compressor": None,
        "model_stats": {},
        "last_error": None,
    }

    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


# ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ (ìºì‹œëœ)
@st.cache_resource
def load_compressor(model_name: str, use_auto_init: bool = False):
    """ëª¨ë¸ ë¡œë”© (ìºì‹œëœ ë¦¬ì†ŒìŠ¤)"""
    try:
        clear_gpu_memory()  # ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
        compressor = EHPCCompressor(model_name, auto_initialize=use_auto_init)
        return compressor, None
    except Exception as e:
        error_msg = f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}"
        return None, error_msg


def get_hardware_info():
    """í•˜ë“œì›¨ì–´ ì •ë³´ ìˆ˜ì§‘"""
    info = {"device": "CPU"}

    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        memory_gb = gpu_props.total_memory / (1024**3)
        allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)
        cached_gb = torch.cuda.memory_reserved(0) / (1024**3)

        info.update(
            {
                "device": "CUDA",
                "gpu_name": gpu_props.name,
                "total_memory": memory_gb,
                "allocated_memory": allocated_gb,
                "cached_memory": cached_gb,
            }
        )
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        info.update({"device": "MPS (Apple Silicon)", "note": "í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©"})

    return info


def show_hf_auth_status():
    """Hugging Face ì¸ì¦ ìƒíƒœ í‘œì‹œ"""
    import os

    # í† í° í™•ì¸
    token = os.getenv("HUGGINGFACE_TOKEN") or os.getenv("HF_TOKEN")

    if token:
        st.success("ğŸ”‘ **Hugging Face ì¸ì¦ë¨**")
    else:
        st.warning("âš ï¸ **Hugging Face ë¯¸ì¸ì¦**")

        with st.expander("ğŸ” ì¸ì¦ ë°©ë²•"):
            st.markdown(
                """
            **Gated ëª¨ë¸ ì ‘ê·¼ì„ ìœ„í•´ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤:**
            
            **ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
            ```bash
            export HUGGINGFACE_TOKEN="your_token_here"
            ```
            
            **ë°©ë²• 2: .env íŒŒì¼ ìƒì„±**
            ```bash
            echo "HUGGINGFACE_TOKEN=your_token_here" > .env
            ```
            
            **ë°©ë²• 3: CLI ë¡œê·¸ì¸**
            ```bash
            huggingface-cli login
            ```
            
            **í† í° ë°œê¸‰**: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
            """
            )

            # ê°„ë‹¨í•œ í† í° ì…ë ¥ UI
            st.subheader("ì„ì‹œ í† í° ì…ë ¥")
            token_input = st.text_input(
                "Hugging Face í† í°", type="password", help="ì´ ì„¸ì…˜ì—ì„œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤"
            )

            if st.button("í† í° ì„¤ì •") and token_input:
                os.environ["HUGGINGFACE_TOKEN"] = token_input
                st.success("âœ… í† í°ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()


def check_model_access(model_name: str) -> bool:
    """ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸"""
    try:
        import os

        from transformers import AutoTokenizer

        token = os.getenv("HUGGINGFACE_TOKEN") or os.getenv("HF_TOKEN")

        # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € ë¡œë”©ìœ¼ë¡œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        AutoTokenizer.from_pretrained(model_name, token=token, trust_remote_code=True)
        return True
    except Exception as e:
        if "gated repo" in str(e).lower():
            return False
        return True  # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì ‘ê·¼ ê°€ëŠ¥í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼


def show_model_selection_sidebar():
    """ì‚¬ì´ë“œë°” ëª¨ë¸ ì„ íƒ UI"""
    with st.sidebar:
        st.header("âš™ï¸ ëª¨ë¸ ì„¤ì •")

        # Hugging Face ì¸ì¦ ìƒíƒœ í™•ì¸
        show_hf_auth_status()

        # í•˜ë“œì›¨ì–´ ì •ë³´ í‘œì‹œ
        hw_info = get_hardware_info()
        if hw_info["device"] == "CUDA":
            st.success(
                f"""
            **ğŸ® GPU ì •ë³´**
            - {hw_info['gpu_name']}
            - ì´ ë©”ëª¨ë¦¬: {hw_info['total_memory']:.1f}GB
            - ì‚¬ìš© ì¤‘: {hw_info['allocated_memory']:.1f}GB
            - ìºì‹œ: {hw_info['cached_memory']:.1f}GB
            """
            )
        elif hw_info["device"] == "MPS":
            st.info(f"**ğŸ Apple Silicon**: {hw_info['note']}")
        else:
            st.warning("**ğŸ’» CPU ëª¨ë“œ**: GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ìë™ ì¶”ì²œ
        recommended_model = get_recommended_model()
        st.info(f"ğŸ¯ **í™˜ê²½ ìµœì í™” ì¶”ì²œ**: `{recommended_model}`")

        # ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ì„ íƒ
        model_categories = {
            "ğŸ¯ ìë™ ì¶”ì²œ": [recommended_model],
            "ğŸ† ìµœê³  ì„±ëŠ¥": [
                "meta-llama/Llama-3.2-3B-Instruct",
                "beomi/KoAlpaca-Polyglot-5.8B",
            ],
            "âš–ï¸ ê· í˜•í˜•": ["Qwen/Qwen2.5-3B-Instruct", "microsoft/Phi-3.5-mini-instruct"],
            "âš¡ ê²½ëŸ‰í˜•": ["google/gemma-2-2b", "microsoft/DialoGPT-medium"],
            "ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŠ¹í™”": [
                "beomi/KoAlpaca-Polyglot-5.8B",
                "Qwen/Qwen2.5-3B-Instruct",
            ],
        }

        selected_category = st.selectbox(
            "ëª¨ë¸ ì¹´í…Œê³ ë¦¬",
            list(model_categories.keys()),
            help="ì‚¬ìš© ëª©ì ì— ë”°ë¥¸ ëª¨ë¸ ë¶„ë¥˜",
        )

        available_models = model_categories[selected_category]
        selected_model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ", available_models, help="ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ì˜ ì¶”ì²œ ëª¨ë¸ë“¤"
        )

        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        model_info = get_model_info(selected_model)
        if model_info:
            korean_stars = "â­" * model_info.get("korean_support", 1)

            # ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸
            access_status = check_model_access(selected_model)
            access_icon = "âœ…" if access_status else "ğŸ”’"

            st.markdown(
                f"""
            **ğŸ“Š ëª¨ë¸ ì •ë³´** {access_icon}
            - **íŒŒë¼ë¯¸í„°**: {model_info.get('params', 'Unknown')}
            - **ìµœì†Œ ë©”ëª¨ë¦¬**: {model_info.get('min_memory_gb', 'Unknown')}GB
            - **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: {model_info.get('context_length', 'Unknown'):,}
            - **í•œêµ­ì–´ ì§€ì›**: {korean_stars} ({model_info.get('korean_support', 1)}/5)
            - **ì¥ì **: {', '.join(model_info.get('pros', []))}
            """
            )

            if not access_status:
                st.error("ğŸ”’ **ì´ ëª¨ë¸ì€ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤!**")
                st.info("ìœ„ì˜ 'ğŸ” ì¸ì¦ ë°©ë²•'ì„ ì°¸ì¡°í•˜ì—¬ Hugging Faceì— ë¡œê·¸ì¸í•˜ì„¸ìš”.")

        # ê³ ê¸‰ ì„¤ì •
        with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
            max_layers = st.slider(
                "ê²€ì‚¬í•  ë ˆì´ì–´ ìˆ˜",
                1,
                6,
                3,
                help="ë” ë§ì€ ë ˆì´ì–´ ê²€ì‚¬ ì‹œ ì •í™•ë„ í–¥ìƒ, ì†ë„ ì €í•˜",
            )
            heads_per_layer = st.slider(
                "ë ˆì´ì–´ë‹¹ í—¤ë“œ ìˆ˜", 1, 4, 2, help="ë” ë§ì€ í—¤ë“œ ì‚¬ìš© ì‹œ í’ˆì§ˆ í–¥ìƒ"
            )
            auto_initialize = st.checkbox(
                "ìë™ ì´ˆê¸°í™”", True, help="ëª¨ë¸ ë¡œë”© ì‹œ ë°”ë¡œ Evaluator Heads ì°¾ê¸°"
            )

        # ì´ˆê¸°í™” ë²„íŠ¼
        col1, col2 = st.columns(2)

        with col1:
            if st.button("ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”", type="primary", use_container_width=True):
                initialize_model(
                    selected_model, max_layers, heads_per_layer, auto_initialize
                )

        with col2:
            if st.button("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬", use_container_width=True):
                clear_gpu_memory()
                if "compressor" in st.session_state:
                    del st.session_state.compressor
                st.cache_resource.clear()
                st.success("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                st.rerun()

        return selected_model, max_layers, heads_per_layer


def initialize_model(model_name, max_layers, heads_per_layer, auto_initialize):
    """ëª¨ë¸ ì´ˆê¸°í™” ì²˜ë¦¬"""
    with st.spinner(f"ğŸ”„ {model_name} ë¡œë”© ì¤‘..."):
        try:
            # ê¸°ì¡´ ëª¨ë¸ê³¼ ë‹¤ë¥´ë©´ ìºì‹œ í´ë¦¬ì–´
            if st.session_state.get("current_model") != model_name:
                clear_gpu_memory()
                st.cache_resource.clear()

            # ëª¨ë¸ ë¡œë”©
            compressor, error = load_compressor(model_name, use_auto_init=False)

            if error:
                st.error(f"âŒ {error}")
                return

            # ì´ˆê¸°í™” ìˆ˜í–‰
            if auto_initialize:
                with st.spinner("ğŸ§  Evaluator Heads ì°¾ëŠ” ì¤‘..."):
                    evaluator_heads = compressor.initialize(
                        max_layers=max_layers, heads_per_layer=heads_per_layer
                    )
            else:
                evaluator_heads = []

            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state.current_model = model_name
            st.session_state.compressor = compressor
            st.session_state.evaluator_heads = evaluator_heads
            st.session_state.initialized = auto_initialize
            st.session_state.model_stats = compressor.get_compression_stats()
            st.session_state.last_error = None

            # ì„±ê³µ ë©”ì‹œì§€
            if auto_initialize:
                st.success(
                    f"âœ… ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™” ì™„ë£Œ! ({len(evaluator_heads)}ê°œ Evaluator Heads ë°œê²¬)"
                )

                # ë°œê²¬ëœ í—¤ë“œ ì •ë³´ í‘œì‹œ
                if evaluator_heads:
                    st.write("**ë°œê²¬ëœ Evaluator Heads:**")
                    for i, head in enumerate(evaluator_heads):
                        st.write(
                            f"  {i+1}. Layer {head.layer}, Head {head.head} "
                            f"(ì‹ ë¢°ë„: {head.confidence_score:.3f})"
                        )
            else:
                st.success("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ìˆ˜ë™ ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        except Exception as e:
            error_msg = f"ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            st.error(f"âŒ {error_msg}")
            st.session_state.last_error = error_msg

            # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´
            if "CUDA" in str(e) or "memory" in str(e).lower():
                st.error(
                    """
                **ğŸ’¡ CUDA/ë©”ëª¨ë¦¬ ì˜¤ë¥˜ í•´ê²° ë°©ë²•:**
                1. 'ë©”ëª¨ë¦¬ ì •ë¦¬' ë²„íŠ¼ í´ë¦­
                2. ë” ì‘ì€ ëª¨ë¸ ì„ íƒ (ê²½ëŸ‰í˜• ì¹´í…Œê³ ë¦¬)
                3. ë¸Œë¼ìš°ì € ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„
                """
                )


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    # í—¤ë”
    st.markdown(
        """
    <div class="main-header">
        <h1>ğŸš€ EHPC: Evaluator Head-based Prompt Compression</h1>
        <p>ì—…ê·¸ë ˆì´ë“œëœ ë…¼ë¬¸ êµ¬í˜„ì²´ - ìµœì‹  ëª¨ë¸ ì§€ì›</p>
    </div>
    """,
        unsafe_allow_html=True,
    )

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    init_session_state()

    # ì‚¬ì´ë“œë°”
    selected_model, max_layers, heads_per_layer = show_model_selection_sidebar()

    # ë©”ì¸ ì»¨í…ì¸ 
    if not st.session_state.initialized:
        show_welcome_screen()
    else:
        show_main_interface()


def show_welcome_screen():
    """ì´ˆê¸° í™”ë©´"""
    st.markdown(
        """
    ## ğŸ‘‹ EHPC ë°ëª¨ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!
    
    ì´ ì—…ê·¸ë ˆì´ë“œëœ ë²„ì „ì€ ë‹¤ìŒê³¼ ê°™ì€ ê°œì„ ì‚¬í•­ì„ ì œê³µí•©ë‹ˆë‹¤:
    
    ### ğŸ†• ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤
    - **ìµœì‹  ëª¨ë¸ ì§€ì›**: Llama 3.2, Qwen 2.5, Gemma 2, Phi 3.5 ë“±
    - **í•œêµ­ì–´ íŠ¹í™”**: í•œêµ­ì–´ ëª¨ë¸ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì§€ì›
    - **ìë™ ìµœì í™”**: í•˜ë“œì›¨ì–´ í™˜ê²½ì— ë§ëŠ” ëª¨ë¸ ìë™ ì¶”ì²œ
    - **ë©”ëª¨ë¦¬ íš¨ìœ¨**: 8bit ì–‘ìí™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°œì„ 
    - **ì•ˆì •ì„± ê°•í™”**: ì˜¤ë¥˜ ì²˜ë¦¬ ë° í´ë°± ì‹œìŠ¤í…œ
    
    ### ğŸ“‹ ì‚¬ìš© ë°©ë²•
    1. **ì‚¬ì´ë“œë°”**ì—ì„œ ì í•©í•œ ëª¨ë¸ ì„ íƒ
    2. **ëª¨ë¸ ì´ˆê¸°í™”** ë²„íŠ¼ìœ¼ë¡œ Evaluator Heads ë°œê²¬
    3. ê° **íƒ­**ì—ì„œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    
    ### âš ï¸ ë¬¸ì œ í•´ê²°
    - **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**: ê²½ëŸ‰í˜• ëª¨ë¸ ì„ íƒ ë˜ëŠ” ë©”ëª¨ë¦¬ ì •ë¦¬
    - **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**: ìë™ ì¶”ì²œ ëª¨ë¸ ì‚¬ìš©
    - **ì„±ëŠ¥ ì €í•˜**: í•˜ë“œì›¨ì–´ì— ë§ëŠ” ì¹´í…Œê³ ë¦¬ ì„ íƒ
    
    ğŸ‘ˆ **ì‹œì‘í•˜ë ¤ë©´ ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ê³  ì´ˆê¸°í™”í•˜ì„¸ìš”!**
    """
    )

    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ë²„íŠ¼
    st.markdown("### ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("âš¡ ê²½ëŸ‰ ëª¨ë¸ë¡œ ë¹ ë¥¸ ì‹œì‘", use_container_width=True):
            initialize_model("google/gemma-2-2b", 2, 1, True)

    with col2:
        if st.button("ğŸ¯ ì¶”ì²œ ëª¨ë¸ë¡œ ì‹œì‘", use_container_width=True):
            recommended = get_recommended_model()
            initialize_model(recommended, 3, 2, True)

    with col3:
        if st.button("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ëª¨ë¸ë¡œ ì‹œì‘", use_container_width=True):
            initialize_model("Qwen/Qwen2.5-3B-Instruct", 3, 2, True)


def show_main_interface():
    """ë©”ì¸ ì¸í„°í˜ì´ìŠ¤"""
    # í˜„ì¬ ëª¨ë¸ ì •ë³´ í‘œì‹œ
    if st.session_state.model_stats:
        stats = st.session_state.model_stats
        model_info = stats.get("model_info", {})

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("í˜„ì¬ ëª¨ë¸", stats.get("model_name", "Unknown").split("/")[-1])
        with col2:
            st.metric("íŒŒë¼ë¯¸í„°", model_info.get("parameters", "Unknown"))
        with col3:
            st.metric("Evaluator Heads", stats.get("num_evaluator_heads", 0))
        with col4:
            st.metric("ë””ë°”ì´ìŠ¤", model_info.get("device", "Unknown"))

    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ’¬ í”„ë¡¬í”„íŠ¸ ì••ì¶•", "ğŸ“Š ë²¤ì¹˜ë§ˆí¬", "ğŸ” ì‹œê°í™”", "ğŸ“ˆ ëª¨ë¸ ì •ë³´"]
    )

    with tab1:
        prompt_compression_tab()

    with tab2:
        benchmark_tab()

    with tab3:
        visualization_tab()

    with tab4:
        model_info_tab()


def prompt_compression_tab():
    """í”„ë¡¬í”„íŠ¸ ì••ì¶• íƒ­"""
    st.header("ğŸ’¬ í”„ë¡¬í”„íŠ¸ ì••ì¶• ë° ìƒì„±")

    # ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    example_prompts = {
        "ì˜ì–´ ì˜ˆì‹œ": """You are a helpful AI assistant with expertise in machine learning and natural language processing. Please analyze the following research paper abstract carefully and provide a comprehensive summary of the key contributions, methodology, and potential applications. Pay special attention to any novel techniques or significant improvements over existing methods.""",
        "í•œêµ­ì–´ ì˜ˆì‹œ": """ì¸ê³µì§€ëŠ¥ê³¼ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ìŒ ì—°êµ¬ ë…¼ë¬¸ì˜ ì´ˆë¡ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ê³  ì£¼ìš” ê¸°ì—¬ì , ë°©ë²•ë¡ , ê·¸ë¦¬ê³  ì ì¬ì  ì‘ìš© ë¶„ì•¼ì— ëŒ€í•œ í¬ê´„ì ì¸ ìš”ì•½ì„ ì œê³µí•´ ì£¼ì„¸ìš”. ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ì¤‘ìš”í•œ ê°œì„  ì‚¬í•­ì— íŠ¹ë³„íˆ ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ ì£¼ì„¸ìš”.""",
        "ê¸°ìˆ  ë¬¸ì„œ": """The transformer architecture has revolutionized natural language processing through its attention mechanism. This technology enables models to focus on relevant parts of the input sequence when generating outputs. Key innovations include multi-head attention, positional encoding, and layer normalization. These components work together to achieve state-of-the-art performance across various NLP tasks.""",
        "ì‚¬ìš©ì ì •ì˜": "",
    }

    selected_example = st.selectbox("ì˜ˆì‹œ ì„ íƒ", list(example_prompts.keys()))

    if selected_example == "ì‚¬ìš©ì ì •ì˜":
        prompt = st.text_area("ì••ì¶•í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", height=150)
    else:
        prompt = st.text_area(
            "ì••ì¶•í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            value=example_prompts[selected_example],
            height=150,
        )

    # ì„¤ì •
    col1, col2, col3 = st.columns(3)
    with col1:
        compression_ratio = st.slider(
            "ì••ì¶• ë¹„ìœ¨", 0.1, 0.9, 0.3, 0.1, help="ìœ ì§€í•  í† í°ì˜ ë¹„ìœ¨"
        )
    with col2:
        max_new_tokens = st.slider(
            "ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜", 50, 300, 100, help="ì‘ë‹µ ê¸¸ì´ ì œí•œ"
        )
    with col3:
        temperature = st.slider(
            "ìƒì„± ì˜¨ë„", 0.1, 1.0, 0.7, 0.1, help="ì‘ë‹µì˜ ì°½ì˜ì„± ì¡°ì ˆ"
        )

    # ê³ ê¸‰ ì˜µì…˜
    with st.expander("ğŸ”§ ê³ ê¸‰ ì˜µì…˜"):
        preserve_special = st.checkbox(
            "íŠ¹ìˆ˜ í† í° ë³´ì¡´", True, help="[CLS], <s> ë“± íŠ¹ìˆ˜ í† í° ìœ ì§€"
        )
        min_tokens = st.number_input(
            "ìµœì†Œ í† í° ìˆ˜", 3, 50, 5, help="ì••ì¶• í›„ ìµœì†Œ ìœ ì§€í•  í† í° ìˆ˜"
        )

    if st.button("ğŸš€ ì••ì¶• ë° ìƒì„± ì‹¤í–‰", type="primary"):
        if not prompt.strip():
            st.warning("âš ï¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return

        with st.spinner("ğŸ”„ ì²˜ë¦¬ ì¤‘..."):
            try:
                compressor = st.session_state.compressor

                start_time = time.time()
                result = compressor.compress_and_generate(
                    prompt,
                    compression_ratio=compression_ratio,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                )
                processing_time = time.time() - start_time

                # ê²°ê³¼ í‘œì‹œ
                st.markdown(
                    '<div class="success-box">âœ… ì••ì¶• ë° ìƒì„± ì™„ë£Œ!</div>',
                    unsafe_allow_html=True,
                )

                # ë©”íŠ¸ë¦­ í‘œì‹œ
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("ì••ì¶•ë¥ ", f"{result['compression_ratio']:.1%}")
                with col2:
                    st.metric("ì›ë³¸ ê¸¸ì´", f"{result['original_length']} ë‹¨ì–´")
                with col3:
                    st.metric("ì••ì¶• ê¸¸ì´", f"{result['compressed_length']} ë‹¨ì–´")
                with col4:
                    st.metric(
                        "í† í° ì ˆì•½", f"{result['tokens_total'] - result['tokens_kept']}"
                    )
                with col5:
                    st.metric("ì²˜ë¦¬ ì‹œê°„", f"{processing_time:.2f}ì´ˆ")

                # ì›ë³¸ vs ì••ì¶• ë¹„êµ
                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("ğŸ“„ ì›ë³¸")
                    st.text_area(
                        "ì›ë³¸ í”„ë¡¬í”„íŠ¸",
                        result["original_text"],
                        height=200,
                        disabled=True,
                        label_visibility="collapsed",
                    )
                    st.subheader("ğŸ¤– ì›ë³¸ ì‘ë‹µ")
                    st.text_area(
                        "ì›ë³¸ ì‘ë‹µ",
                        result["original_response"],
                        height=150,
                        disabled=True,
                        label_visibility="collapsed",
                    )
                    st.caption(
                        f"ìƒì„± ì‹œê°„: {result.get('generation_time_original', 0):.2f}ì´ˆ"
                    )

                with col2:
                    st.subheader("ğŸ“„ ì••ì¶•ë¨")
                    st.text_area(
                        "ì••ì¶• í”„ë¡¬í”„íŠ¸",
                        result["compressed_text"],
                        height=200,
                        disabled=True,
                        label_visibility="collapsed",
                    )
                    st.subheader("ğŸ¤– ì••ì¶• ì‘ë‹µ")
                    st.text_area(
                        "ì••ì¶• ì‘ë‹µ",
                        result["compressed_response"],
                        height=150,
                        disabled=True,
                        label_visibility="collapsed",
                    )
                    st.caption(
                        f"ìƒì„± ì‹œê°„: {result.get('generation_time_compressed', 0):.2f}ì´ˆ"
                    )

                # ì¶”ê°€ ë¶„ì„
                with st.expander("ğŸ“Š ìƒì„¸ ë¶„ì„"):
                    st.json(
                        {
                            "ì••ì¶• í†µê³„": {
                                "ì›ë³¸ í† í° ìˆ˜": result["tokens_total"],
                                "ì••ì¶• í† í° ìˆ˜": result["tokens_kept"],
                                "ì œê±°ëœ í† í°": result["tokens_total"]
                                - result["tokens_kept"],
                                "ì••ì¶•ë¥ ": f"{result['compression_ratio']:.1%}",
                            },
                            "ì„±ëŠ¥ ë¶„ì„": {
                                "ì›ë³¸ ì…ë ¥ í† í°": result.get(
                                    "original_tokens_count", "Unknown"
                                ),
                                "ì••ì¶• ì…ë ¥ í† í°": result.get(
                                    "compressed_tokens_count", "Unknown"
                                ),
                                "ì›ë³¸ ìƒì„± ì‹œê°„": f"{result.get('generation_time_original', 0):.3f}ì´ˆ",
                                "ì••ì¶• ìƒì„± ì‹œê°„": f"{result.get('generation_time_compressed', 0):.3f}ì´ˆ",
                            },
                        }
                    )

            except Exception as e:
                st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

                # ì˜¤ë¥˜ ìœ í˜•ë³„ í•´ê²°ì±… ì œì•ˆ
                error_str = str(e).lower()
                if "cuda" in error_str or "memory" in error_str:
                    st.error(
                        """
                    **ğŸ’¡ ë©”ëª¨ë¦¬ ì˜¤ë¥˜ í•´ê²° ë°©ë²•:**
                    1. ì‚¬ì´ë“œë°”ì—ì„œ 'ë©”ëª¨ë¦¬ ì •ë¦¬' ë²„íŠ¼ í´ë¦­
                    2. ë” ë‚®ì€ ì••ì¶• ë¹„ìœ¨ ì‹œë„ (0.5 ì´ìƒ)
                    3. ë” ì‘ì€ ëª¨ë¸ë¡œ ë³€ê²½
                    """
                    )
                elif "token" in error_str:
                    st.error(
                        """
                    **ğŸ’¡ í† í° ì˜¤ë¥˜ í•´ê²° ë°©ë²•:**
                    1. ë” ì§§ì€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    2. ìµœì†Œ í† í° ìˆ˜ ì¤„ì´ê¸°
                    3. ë‹¤ë¥¸ ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ ì‹œë„
                    """
                    )


def benchmark_tab():
    """ë²¤ì¹˜ë§ˆí¬ íƒ­"""
    st.header("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ í‰ê°€")

    st.info("ë‹¤ì–‘í•œ ì••ì¶• ë¹„ìœ¨ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.")

    # ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
    col1, col2 = st.columns(2)
    with col1:
        benchmark_type = st.selectbox(
            "ë²¤ì¹˜ë§ˆí¬ ìœ í˜•",
            ["QA (SQuAD)", "ìš”ì•½ (CNN/DailyMail)", "ì••ì¶• ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"],
            help="í‰ê°€í•  íƒœìŠ¤í¬ ì„ íƒ",
        )
    with col2:
        num_samples = st.slider("ìƒ˜í”Œ ìˆ˜", 5, 50, 10, help="í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ê°œìˆ˜")

    compression_ratios = st.multiselect(
        "í…ŒìŠ¤íŠ¸í•  ì••ì¶• ë¹„ìœ¨",
        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],
        default=[0.3, 0.5, 0.7],
        help="ì—¬ëŸ¬ ì••ì¶• ë¹„ìœ¨ì—ì„œ ì„±ëŠ¥ ë¹„êµ",
    )

    if st.button("ğŸ§ª ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"):
        if not compression_ratios:
            st.warning("âš ï¸ ì••ì¶• ë¹„ìœ¨ì„ ìµœì†Œ í•˜ë‚˜ ì„ íƒí•´ì£¼ì„¸ìš”!")
            return

        progress_bar = st.progress(0)
        status_text = st.empty()

        try:
            compressor = st.session_state.compressor

            if benchmark_type == "ì••ì¶• ì„±ëŠ¥ í…ŒìŠ¤íŠ¸":
                # ì••ì¶• ì„±ëŠ¥ ìì²´ í…ŒìŠ¤íŠ¸
                test_texts = [
                    "ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ í†µí•´ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                    "Natural language processing has revolutionized how we interact with computers and analyze text data.",
                    "The attention mechanism in transformers allows models to focus on relevant parts of the input sequence.",
                    "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, êµ¬ë¬¸ ë¶„ì„, ì˜ë¯¸ ë¶„ì„ ë“± ë‹¤ì–‘í•œ ë‹¨ê³„ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.",
                    "Large language models like GPT and BERT have achieved remarkable performance across various NLP tasks.",
                ]

                status_text.text("ì••ì¶• ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
                results = compressor.benchmark_compression(
                    test_texts, compression_ratios
                )

                # ê²°ê³¼ í‘œì‹œ
                st.success("âœ… ì••ì¶• ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

                # ê²°ê³¼ í…Œì´ë¸”
                import pandas as pd

                df_data = []
                for ratio, metrics in results.items():
                    df_data.append(
                        {
                            "ì••ì¶•ë¥ ": f"{float(ratio):.1%}",
                            "í‰ê·  ì‹¤ì œ ì••ì¶•ë¥ ": f"{metrics['avg_actual_ratio']:.1%}",
                            "í‰ê·  ì²˜ë¦¬ ì‹œê°„": f"{metrics['avg_compression_time']:.3f}ì´ˆ",
                            "í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ": len(metrics["results"]),
                        }
                    )

                df = pd.DataFrame(df_data)
                st.dataframe(df, use_container_width=True)

                # ìƒì„¸ ê²°ê³¼
                with st.expander("ğŸ“ˆ ìƒì„¸ ê²°ê³¼"):
                    for ratio, metrics in results.items():
                        st.write(f"**ì••ì¶•ë¥  {float(ratio):.1%} ê²°ê³¼:**")
                        for i, result in enumerate(metrics["results"]):
                            st.write(
                                f"  ìƒ˜í”Œ {i+1}: {result['original_length']}â†’{result['compressed_length']} í† í° "
                                f"({result['actual_ratio']:.1%}, {result['compression_time']:.3f}ì´ˆ)"
                            )

            else:
                # ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ (QA, ìš”ì•½)
                with st.spinner("ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘... (ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                    benchmark = EHPCBenchmark(compressor)

                    if benchmark_type == "QA (SQuAD)":
                        status_text.text("SQuAD QA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
                        results = benchmark.run_qa_benchmark(
                            num_samples=num_samples,
                            compression_ratios=compression_ratios,
                        )
                    else:
                        status_text.text("CNN/DailyMail ìš”ì•½ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
                        results = benchmark.run_summarization_benchmark(
                            num_samples=num_samples,
                            compression_ratios=compression_ratios,
                        )

                    progress_bar.progress(100)

                    # ê²°ê³¼ í‘œì‹œ
                    st.success("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

                    # ê²°ê³¼ í…Œì´ë¸”
                    import pandas as pd

                    df_data = []
                    for ratio, metrics in results.items():
                        row = {"ì••ì¶•ë¥ ": f"{float(ratio):.1%}"}
                        row.update(
                            {
                                k.upper(): f"{v:.3f}"
                                for k, v in metrics.items()
                                if k != "samples" and isinstance(v, (int, float))
                            }
                        )
                        row["ìƒ˜í”Œ ìˆ˜"] = metrics.get("samples", "Unknown")
                        df_data.append(row)

                    df = pd.DataFrame(df_data)
                    st.dataframe(df, use_container_width=True)

                    # ì‹œê°í™”
                    if len(results) > 1:
                        try:
                            visualizer = AttentionVisualizer()
                            fig = visualizer.plot_benchmark_results(results)
                            st.plotly_chart(fig, use_container_width=True)
                        except Exception as e:
                            st.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

        except Exception as e:
            st.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {str(e)}")
            progress_bar.empty()
            status_text.empty()


def visualization_tab():
    """ì‹œê°í™” íƒ­"""
    st.header("ğŸ” Attention ì‹œê°í™”")

    st.info("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ í† í° ì¤‘ìš”ë„ì™€ attention íŒ¨í„´ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.")

    # ì˜ˆì‹œ í…ìŠ¤íŠ¸ ì„ íƒ
    viz_examples = {
        "ì˜ì–´ ì˜ˆì‹œ": "The quick brown fox jumps over the lazy dog in the beautiful garden.",
        "í•œêµ­ì–´ ì˜ˆì‹œ": "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ì•„ë¦„ë‹¤ìš´ ì •ì›ì—ì„œ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.",
        "ê¸°ìˆ  í…ìŠ¤íŠ¸": "Machine learning algorithms analyze patterns in data to make predictions.",
        "ì‚¬ìš©ì ì •ì˜": "",
    }

    selected_viz_example = st.selectbox("ì‹œê°í™” ì˜ˆì‹œ ì„ íƒ", list(viz_examples.keys()))

    if selected_viz_example == "ì‚¬ìš©ì ì •ì˜":
        viz_prompt = st.text_area("ì‹œê°í™”í•  í…ìŠ¤íŠ¸:", height=100)
    else:
        viz_prompt = st.text_area(
            "ì‹œê°í™”í•  í…ìŠ¤íŠ¸:", value=viz_examples[selected_viz_example], height=100
        )

    viz_ratio = st.slider("ì‹œê°í™”ìš© ì••ì¶• ë¹„ìœ¨", 0.1, 0.9, 0.5, 0.1)

    if st.button("ğŸ¨ ì‹œê°í™” ìƒì„±"):
        if not viz_prompt.strip():
            st.warning("âš ï¸ ì‹œê°í™”í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return

        with st.spinner("ğŸ” ë¶„ì„ ì¤‘..."):
            try:
                compressor = st.session_state.compressor

                # ì••ì¶• ê²°ê³¼ ì–»ê¸°
                compression_result = compressor.compress_prompt(
                    viz_prompt, compression_ratio=viz_ratio
                )

                # ì‹œê°í™” ìƒì„±
                visualizer = AttentionVisualizer()

                # í† í° ì¤‘ìš”ë„ ì‹œê°í™”
                fig_importance = visualizer.plot_token_importance(
                    compression_result.original_tokens,
                    compression_result.token_scores,
                    compression_result.selected_indices,
                    title="í† í° ì¤‘ìš”ë„ ì ìˆ˜ (ë¹¨ê°„ìƒ‰: ì„ íƒëœ í† í°)",
                )
                st.plotly_chart(fig_importance, use_container_width=True)

                # ì••ì¶• í†µê³„
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ì´ í† í°", len(compression_result.original_tokens))
                with col2:
                    st.metric("ì„ íƒëœ í† í°", len(compression_result.selected_indices))
                with col3:
                    st.metric(
                        "ì‹¤ì œ ì••ì¶•ë¥ ", f"{compression_result.compression_ratio:.1%}"
                    )
                with col4:
                    st.metric(
                        "Evaluator Heads", len(compression_result.evaluator_heads)
                    )

                # ì••ì¶• ê²°ê³¼ í…ìŠ¤íŠ¸ ë¹„êµ
                compressed_text = compressor.tokens_to_text(
                    compression_result.compressed_tokens
                )

                st.subheader("ğŸ“„ ì••ì¶• ê²°ê³¼ ë¹„êµ")
                col1, col2 = st.columns(2)
                with col1:
                    st.text_area(
                        "ì›ë³¸ í…ìŠ¤íŠ¸",
                        viz_prompt,
                        height=100,
                        disabled=True,
                        label_visibility="collapsed",
                    )
                with col2:
                    st.text_area(
                        "ì••ì¶•ëœ í…ìŠ¤íŠ¸",
                        compressed_text,
                        height=100,
                        disabled=True,
                        label_visibility="collapsed",
                    )

                # í† í°ë³„ ìƒì„¸ ì •ë³´
                with st.expander("ğŸ” í† í°ë³„ ìƒì„¸ ë¶„ì„"):
                    df_data = []
                    for i, (token, score) in enumerate(
                        zip(
                            compression_result.original_tokens,
                            compression_result.token_scores,
                        )
                    ):
                        df_data.append(
                            {
                                "ìœ„ì¹˜": i,
                                "í† í°": token,
                                "ì¤‘ìš”ë„ ì ìˆ˜": f"{score:.4f}",
                                "ì„ íƒë¨": (
                                    "âœ…"
                                    if i in compression_result.selected_indices
                                    else "âŒ"
                                ),
                            }
                        )

                    import pandas as pd

                    df = pd.DataFrame(df_data)
                    st.dataframe(df, use_container_width=True)

                # ë¦¬í¬íŠ¸ ìƒì„±
                try:
                    report = visualizer.create_compression_report(compression_result)
                    with st.expander("ğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸"):
                        st.markdown(report)
                except Exception as e:
                    st.warning(f"âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

            except Exception as e:
                st.error(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {str(e)}")


def model_info_tab():
    """ëª¨ë¸ ì •ë³´ íƒ­"""
    st.header("ğŸ“ˆ ëª¨ë¸ ì •ë³´ ë° í†µê³„")

    if st.session_state.model_stats:
        stats = st.session_state.model_stats
        model_info = stats.get("model_info", {})

        # ê¸°ë³¸ ì •ë³´
        st.subheader("ğŸ“Š ê¸°ë³¸ ì •ë³´")
        col1, col2 = st.columns(2)

        with col1:
            st.info(
                f"""
            **ëª¨ë¸ ì´ë¦„**: {stats.get('model_name', 'Unknown')}
            **íŒŒë¼ë¯¸í„°**: {model_info.get('parameters', 'Unknown')}
            **ë””ë°”ì´ìŠ¤**: {model_info.get('device', 'Unknown')}
            **ì–‘ìí™”**: {'âœ…' if model_info.get('quantized', False) else 'âŒ'}
            """
            )

        with col2:
            st.info(
                f"""
            **ë ˆì´ì–´ ìˆ˜**: {model_info.get('num_layers', 'Unknown')}
            **Attention Heads**: {model_info.get('num_heads', 'Unknown')}
            **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: {model_info.get('context_length', 'Unknown'):,}
            **í•œêµ­ì–´ ì§€ì›**: {'â­' * model_info.get('korean_support', 1)} ({model_info.get('korean_support', 1)}/5)
            """
            )

        # Evaluator Heads ì •ë³´
        if stats.get("evaluator_heads"):
            st.subheader("ğŸ§  Evaluator Heads ì •ë³´")

            heads_data = []
            for head in stats["evaluator_heads"]:
                heads_data.append(
                    {
                        "ë ˆì´ì–´": head["layer"],
                        "í—¤ë“œ": head["head"],
                        "ì‹ ë¢°ë„": f"{head['confidence']:.4f}",
                        "ì„ íƒì„±": f"{head['selectivity']:.4f}",
                        "ì¢…í•© ì ìˆ˜": f"{head['confidence'] + (1-head['selectivity']):.4f}",
                    }
                )

            import pandas as pd

            df = pd.DataFrame(heads_data)
            st.dataframe(df, use_container_width=True)

            # í—¤ë“œ ë¶„í¬ ì‹œê°í™”
            try:
                import plotly.express as px

                layer_counts = {}
                for head in stats["evaluator_heads"]:
                    layer = head["layer"]
                    layer_counts[layer] = layer_counts.get(layer, 0) + 1

                if layer_counts:
                    fig = px.bar(
                        x=list(layer_counts.keys()),
                        y=list(layer_counts.values()),
                        title="ë ˆì´ì–´ë³„ Evaluator Heads ë¶„í¬",
                        labels={"x": "ë ˆì´ì–´", "y": "í—¤ë“œ ìˆ˜"},
                    )
                    st.plotly_chart(fig, use_container_width=True)

            except Exception as e:
                st.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        st.subheader("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
        memory_info = model_info.get("memory_usage", "Unknown")
        st.code(memory_info)

        # ëª¨ë¸ ì¥ë‹¨ì 
        if model_info.get("pros"):
            st.subheader("âœ… ëª¨ë¸ ì¥ì ")
            for pro in model_info["pros"]:
                st.write(f"â€¢ {pro}")

        # í•˜ë“œì›¨ì–´ ì •ë³´
        st.subheader("ğŸ–¥ï¸ í•˜ë“œì›¨ì–´ ì •ë³´")
        hw_info = get_hardware_info()
        st.json(hw_info)

        # ìˆ˜ë™ ì´ˆê¸°í™” ì˜µì…˜
        if not st.session_state.initialized:
            st.subheader("ğŸ”§ ìˆ˜ë™ ì´ˆê¸°í™”")
            st.warning("âš ï¸ Evaluator Headsê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            col1, col2 = st.columns(2)
            with col1:
                manual_layers = st.slider("ë ˆì´ì–´ ìˆ˜", 1, 6, 3, key="manual_layers")
            with col2:
                manual_heads = st.slider("í—¤ë“œ ìˆ˜", 1, 4, 2, key="manual_heads")

            if st.button("ğŸš€ ìˆ˜ë™ ì´ˆê¸°í™” ì‹¤í–‰", type="primary"):
                with st.spinner("ğŸ§  Evaluator Heads ì°¾ëŠ” ì¤‘..."):
                    try:
                        compressor = st.session_state.compressor
                        evaluator_heads = compressor.initialize(
                            max_layers=manual_layers, heads_per_layer=manual_heads
                        )

                        st.session_state.evaluator_heads = evaluator_heads
                        st.session_state.initialized = True
                        st.session_state.model_stats = (
                            compressor.get_compression_stats()
                        )

                        st.success(
                            f"âœ… ì´ˆê¸°í™” ì™„ë£Œ! {len(evaluator_heads)}ê°œ Evaluator Heads ë°œê²¬"
                        )
                        st.rerun()

                    except Exception as e:
                        st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    else:
        st.warning(
            "âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ê³  ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”."
        )

        # ì§€ì› ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
        st.subheader("ğŸ“‹ ì§€ì›ë˜ëŠ” ëª¨ë¸ë“¤")

        models_data = []
        for model_name in list_supported_models():
            info = get_model_info(model_name)
            models_data.append(
                {
                    "ëª¨ë¸ëª…": model_name,
                    "íŒŒë¼ë¯¸í„°": info.get("params", "Unknown"),
                    "ìµœì†Œ ë©”ëª¨ë¦¬": f"{info.get('min_memory_gb', 'Unknown')}GB",
                    "í•œêµ­ì–´ ì§€ì›": "â­" * info.get("korean_support", 1),
                    "ì¥ì ": ", ".join(info.get("pros", [])[:2]),  # ì²˜ìŒ 2ê°œë§Œ
                }
            )

        import pandas as pd

        df = pd.DataFrame(models_data)
        st.dataframe(df, use_container_width=True)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜¤ë¥˜: {str(e)}")
        st.error("ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
        st.code(traceback.format_exc())

        if st.button("ğŸ”„ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()
