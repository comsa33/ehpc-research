import os
import sys

import streamlit as st

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import time
import torch
import gc

from core.prompt_compressor import EHPCCompressor
from evaluation.benchmarks import EHPCBenchmark
from visualization.attention_viz import AttentionVisualizer

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="EHPC Demo",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded",
)

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜
def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

# ì‹±ê¸€í„´ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ê´€ë¦¬
@st.cache_resource
def get_compressor(model_name: str):
    """ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ëŠ” ì‹±ê¸€í„´ íŒ¨í„´"""
    clear_gpu_memory()  # ì´ˆê¸°í™” ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
    return EHPCCompressor(model_name)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë” ì•ˆì „í•œ ë°©ì‹)
def init_session_state():
    if "current_model" not in st.session_state:
        st.session_state.current_model = None
    if "initialized" not in st.session_state:
        st.session_state.initialized = False
    if "evaluator_heads" not in st.session_state:
        st.session_state.evaluator_heads = []

def main():
    st.title("ğŸš€ EHPC: Evaluator Head-based Prompt Compression")
    st.markdown("ë…¼ë¬¸ 'Efficient Prompt Compression with Evaluator Heads' ì˜ êµ¬í˜„ì²´")
    
    init_session_state()

    # ì‚¬ì´ë“œë°” - ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        model_name = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            [
                "microsoft/DialoGPT-medium",
                "microsoft/DialoGPT-small", 
                "gpt2",
                "gpt2-medium",
            ],
            help="attention weightsì— ì ‘ê·¼ ê°€ëŠ¥í•œ HuggingFace ëª¨ë¸",
        )

        max_layers = st.slider("ê²€ì‚¬í•  ë ˆì´ì–´ ìˆ˜", 1, 6, 3)
        heads_per_layer = st.slider("ë ˆì´ì–´ë‹¹ í—¤ë“œ ìˆ˜", 1, 4, 2)
        
        # GPU ìƒíƒœ í‘œì‹œ
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
            gpu_cached = torch.cuda.memory_reserved(0) / 1024**3
            
            st.info(f"""
            **GPU ìƒíƒœ**
            - ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}
            - ì´ ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB
            - ì‚¬ìš©ì¤‘: {gpu_allocated:.1f}GB
            - ìºì‹œë¨: {gpu_cached:.1f}GB
            """)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ ë²„íŠ¼
        if st.button("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬"):
            clear_gpu_memory()
            st.success("âœ… GPU ë©”ëª¨ë¦¬ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

        if st.button("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", type="primary"):
            with st.spinner("Evaluator Headsë¥¼ ì°¾ëŠ” ì¤‘..."):
                try:
                    # ëª¨ë¸ ë³€ê²½ ì‹œ ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬
                    if (st.session_state.current_model is not None and 
                        st.session_state.current_model != model_name):
                        clear_gpu_memory()
                        st.cache_resource.clear()  # ìºì‹œëœ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                    
                    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ CUDA ë””ë²„ê¹… í™œì„±í™”
                    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
                    
                    # ì‹±ê¸€í„´ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ë¡œë”©
                    compressor = get_compressor(model_name)
                    
                    # ì´ˆê¸°í™” ìˆ˜í–‰
                    evaluator_heads = compressor.initialize(
                        max_layers=max_layers, heads_per_layer=heads_per_layer
                    )
                    
                    # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                    st.session_state.current_model = model_name
                    st.session_state.initialized = True
                    st.session_state.evaluator_heads = evaluator_heads
                    
                    st.success(f"âœ… {len(evaluator_heads)}ê°œì˜ Evaluator Heads ë°œê²¬!")

                    # ë°œê²¬ëœ í—¤ë“œë“¤ í‘œì‹œ
                    for head in evaluator_heads:
                        st.info(
                            f"Layer {head.layer}, Head {head.head} (ì‹ ë¢°ë„: {head.confidence_score:.3f})"
                        )

                except Exception as e:
                    st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # ì—ëŸ¬ ë°œìƒ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
                    clear_gpu_memory()
                    # ìƒì„¸ ì—ëŸ¬ ì •ë³´ í‘œì‹œ
                    if "CUDA" in str(e):
                        st.error("""
                        **CUDA ì—ëŸ¬ í•´ê²° ë°©ë²•:**
                        1. 'GPU ë©”ëª¨ë¦¬ ì •ë¦¬' ë²„íŠ¼ í´ë¦­
                        2. ë” ì‘ì€ ëª¨ë¸ ì„ íƒ (DialoGPT-small)
                        3. ë¸Œë¼ìš°ì € ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„
                        """)

    # ë©”ì¸ ì»¨í…ì¸ 
    if not st.session_state.initialized:
        st.warning("âš ï¸ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”!")
        
        # ì´ˆê¸°í™” ì•ˆë‚´
        st.info("""
        **ğŸ“‹ ì‚¬ìš© ë°©ë²•:**
        1. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì„ íƒ
        2. 'ì‹œìŠ¤í…œ ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ Evaluator Heads ë°œê²¬
        3. ê° íƒ­ì—ì„œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        
        **âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:**
        - ë” ì‘ì€ ëª¨ë¸ (DialoGPT-small) ì„ íƒ
        - 'GPU ë©”ëª¨ë¦¬ ì •ë¦¬' ë²„íŠ¼ ì‚¬ìš©
        - ë¸Œë¼ìš°ì € ìƒˆë¡œê³ ì¹¨
        """)
        return

    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ í”„ë¡¬í”„íŠ¸ ì••ì¶•", "ğŸ“Š ë²¤ì¹˜ë§ˆí¬", "ğŸ” ì‹œê°í™”"])

    with tab1:
        prompt_compression_tab()

    with tab2:
        benchmark_tab()

    with tab3:
        visualization_tab()


def prompt_compression_tab():
    st.header("ğŸ’¬ í”„ë¡¬í”„íŠ¸ ì••ì¶• ë° ìƒì„±")

    # í”„ë¡¬í”„íŠ¸ ì…ë ¥
    prompt = st.text_area(
        "ì••ì¶•í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        value="""You are a helpful AI assistant with expertise in machine learning and natural language processing. Please analyze the following research paper abstract carefully and provide a comprehensive summary of the key contributions, methodology, and potential applications. Pay special attention to any novel techniques or significant improvements over existing methods. Your analysis should be detailed and technically accurate while remaining accessible to researchers in the field.""",
        height=150,
    )

    col1, col2 = st.columns(2)
    with col1:
        compression_ratio = st.slider("ì••ì¶• ë¹„ìœ¨", 0.1, 0.9, 0.3, 0.1)
    with col2:
        max_new_tokens = st.slider("ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜", 50, 200, 100)

    if st.button("ğŸš€ ì••ì¶• ë° ìƒì„± ì‹¤í–‰", type="primary"):
        with st.spinner("ì²˜ë¦¬ ì¤‘..."):
            try:
                # í˜„ì¬ ëª¨ë¸ì˜ ì••ì¶•ê¸° ê°€ì ¸ì˜¤ê¸°
                compressor = get_compressor(st.session_state.current_model)
                
                start_time = time.time()
                result = compressor.compress_and_generate(
                    prompt,
                    compression_ratio=compression_ratio,
                    max_new_tokens=max_new_tokens,
                )
                processing_time = time.time() - start_time

                # ê²°ê³¼ í‘œì‹œ
                st.success(f"âœ… ì™„ë£Œ! (ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ)")

                # ë©”íŠ¸ë¦­ í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ì••ì¶•ë¥ ", f"{result['compression_ratio']:.1%}")
                with col2:
                    st.metric("ì›ë³¸ ê¸¸ì´", f"{result['original_length']} ë‹¨ì–´")
                with col3:
                    st.metric("ì••ì¶• ê¸¸ì´", f"{result['compressed_length']} ë‹¨ì–´")
                with col4:
                    st.metric(
                        "í† í° ì ˆì•½",
                        f"{result['tokens_total'] - result['tokens_kept']} ê°œ",
                    )

                # ì›ë³¸ vs ì••ì¶• ë¹„êµ
                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("ğŸ“„ ì›ë³¸ í”„ë¡¬í”„íŠ¸")
                    st.text_area(
                        "ì›ë³¸ í…ìŠ¤íŠ¸",
                        value=result["original_text"],
                        height=200,
                        disabled=True,
                        label_visibility="collapsed",
                    )
                    st.subheader("ğŸ¤– ì›ë³¸ ì‘ë‹µ")
                    st.text_area(
                        "ì›ë³¸ ì‘ë‹µ",
                        value=result["original_response"],
                        height=150,
                        disabled=True,
                        label_visibility="collapsed",
                    )

                with col2:
                    st.subheader("ğŸ“„ ì••ì¶•ëœ í”„ë¡¬í”„íŠ¸")
                    st.text_area(
                        "ì••ì¶•ëœ í…ìŠ¤íŠ¸",
                        value=result["compressed_text"],
                        height=200,
                        disabled=True,
                        label_visibility="collapsed",
                    )
                    st.subheader("ğŸ¤– ì••ì¶•ëœ ì‘ë‹µ")
                    st.text_area(
                        "ì••ì¶•ëœ ì‘ë‹µ",
                        value=result["compressed_response"],
                        height=150,
                        disabled=True,
                        label_visibility="collapsed",
                    )

            except Exception as e:
                st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                if "CUDA" in str(e) or "memory" in str(e).lower():
                    if st.button("ğŸ§¹ ì—ëŸ¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬"):
                        clear_gpu_memory()
                        st.rerun()


def benchmark_tab():
    st.header("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ í‰ê°€")

    st.info("ë‹¤ì–‘í•œ ì••ì¶• ë¹„ìœ¨ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.")

    col1, col2 = st.columns(2)
    with col1:
        benchmark_type = st.selectbox(
            "ë²¤ì¹˜ë§ˆí¬ ìœ í˜•", ["QA (SQuAD)", "ìš”ì•½ (CNN/DailyMail)"]
        )
    with col2:
        num_samples = st.slider("ìƒ˜í”Œ ìˆ˜", 10, 100, 20)

    compression_ratios = st.multiselect(
        "í…ŒìŠ¤íŠ¸í•  ì••ì¶• ë¹„ìœ¨", [0.2, 0.3, 0.4, 0.5, 0.6, 0.7], default=[0.3, 0.5, 0.7]
    )

    if st.button("ğŸ§ª ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"):
        if not compression_ratios:
            st.warning("ì••ì¶• ë¹„ìœ¨ì„ ìµœì†Œ í•˜ë‚˜ ì„ íƒí•´ì£¼ì„¸ìš”!")
            return

        with st.spinner("ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘... (ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            try:
                # í˜„ì¬ ëª¨ë¸ì˜ ì••ì¶•ê¸° ê°€ì ¸ì˜¤ê¸°
                compressor = get_compressor(st.session_state.current_model)
                benchmark = EHPCBenchmark(compressor)

                if benchmark_type == "QA (SQuAD)":
                    results = benchmark.run_qa_benchmark(
                        num_samples=num_samples, compression_ratios=compression_ratios
                    )
                else:
                    results = benchmark.run_summarization_benchmark(
                        num_samples=num_samples, compression_ratios=compression_ratios
                    )

                # ê²°ê³¼ í‘œì‹œ
                st.success("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

                # ê²°ê³¼ í…Œì´ë¸”
                st.subheader("ğŸ“ˆ ì„±ëŠ¥ ê²°ê³¼")

                import pandas as pd

                df_data = []
                for ratio, metrics in results.items():
                    row = {"ì••ì¶•ë¥ ": f"{ratio:.1%}"}
                    row.update(
                        {
                            k.upper(): f"{v:.3f}"
                            for k, v in metrics.items()
                            if k != "samples"
                        }
                    )
                    df_data.append(row)

                df = pd.DataFrame(df_data)
                st.dataframe(df, use_container_width=True)

                # ì‹œê°í™”
                visualizer = AttentionVisualizer()
                fig = visualizer.plot_benchmark_results(results)
                st.plotly_chart(fig, use_container_width=True)

            except Exception as e:
                st.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
                if "CUDA" in str(e) or "memory" in str(e).lower():
                    if st.button("ğŸ§¹ ë²¤ì¹˜ë§ˆí¬ ì—ëŸ¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬"):
                        clear_gpu_memory()
                        st.rerun()


def visualization_tab():
    st.header("ğŸ” Attention ì‹œê°í™”")

    st.info("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ í† í° ì¤‘ìš”ë„ì™€ attention íŒ¨í„´ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.")

    viz_prompt = st.text_area(
        "ì‹œê°í™”í•  í…ìŠ¤íŠ¸:",
        value="The quick brown fox jumps over the lazy dog in the beautiful garden.",
        height=100,
    )

    viz_ratio = st.slider("ì‹œê°í™”ìš© ì••ì¶• ë¹„ìœ¨", 0.1, 0.9, 0.5, 0.1)

    if st.button("ğŸ¨ ì‹œê°í™” ìƒì„±"):
        with st.spinner("ë¶„ì„ ì¤‘..."):
            try:
                # í˜„ì¬ ëª¨ë¸ì˜ ì••ì¶•ê¸° ê°€ì ¸ì˜¤ê¸°
                compressor = get_compressor(st.session_state.current_model)
                
                # ì••ì¶• ê²°ê³¼ ì–»ê¸°
                compression_result = compressor.compress_prompt(
                    viz_prompt, compression_ratio=viz_ratio
                )

                visualizer = AttentionVisualizer()

                # í† í° ì¤‘ìš”ë„ ì‹œê°í™”
                fig_importance = visualizer.plot_token_importance(
                    compression_result.original_tokens,
                    compression_result.token_scores,
                    compression_result.selected_indices,
                    title="í† í° ì¤‘ìš”ë„ ì ìˆ˜ (ë¹¨ê°„ìƒ‰: ì„ íƒëœ í† í°)",
                )
                st.plotly_chart(fig_importance, use_container_width=True)

                # ì••ì¶• í†µê³„
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì´ í† í°", len(compression_result.original_tokens))
                with col2:
                    st.metric("ì„ íƒëœ í† í°", len(compression_result.selected_indices))
                with col3:
                    st.metric(
                        "ì‹¤ì œ ì••ì¶•ë¥ ", f"{compression_result.compression_ratio:.1%}"
                    )

                # ì••ì¶• ê²°ê³¼ í…ìŠ¤íŠ¸
                compressed_text = compressor.tokens_to_text(
                    compression_result.compressed_tokens
                )

                st.subheader("ğŸ“„ ì••ì¶• ê²°ê³¼")
                col1, col2 = st.columns(2)
                with col1:
                    st.text_area("ì›ë³¸ í…ìŠ¤íŠ¸", viz_prompt, height=100, disabled=True, label_visibility="collapsed")
                with col2:
                    st.text_area("ì••ì¶•ëœ í…ìŠ¤íŠ¸", compressed_text, height=100, disabled=True, label_visibility="collapsed")

                # ë¦¬í¬íŠ¸ ìƒì„±
                report = visualizer.create_compression_report(compression_result)
                with st.expander("ğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸"):
                    st.markdown(report)

            except Exception as e:
                st.error(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
                if "CUDA" in str(e) or "memory" in str(e).lower():
                    if st.button("ğŸ§¹ ì‹œê°í™” ì—ëŸ¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬"):
                        clear_gpu_memory()
                        st.rerun()


if __name__ == "__main__":
    main()
